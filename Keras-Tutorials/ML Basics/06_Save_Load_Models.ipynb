{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Tutorials\n",
    "# ML Basics 06: Save and Load Models\n",
    "\n",
    "Can save models during and after training. This means we don't have to start training models from scratch every time. We can pick up where we left aoff. \n",
    "\n",
    "Also, we can publish the weights of the trained model along with code to create the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# For manipulating filepaths\n",
    "import os\n",
    "\n",
    "# TF and Keras API\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python module for importing/exporting weights of trained models as h5 files\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiting ourselves to the first 1000 samples\n",
    "train_labels = train_labels[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "# Reshape the images as 28 * 28 dimensional vectors, and normalize to [0, 1] range\n",
    "train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
    "test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple sequential model\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Input layer and dropout - 784 because 28 * 28\n",
    "        keras.layers.Dense(512, activation='relu', input_shape=(784, )), \n",
    "        keras.layers.Dropout(0.2), \n",
    "        \n",
    "        # No hidden layer\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(optimizer='adam',\n",
    "         loss='sparse_categorical_crossentropy', \n",
    "         metrics=['accuracy'])\n",
    "    \n",
    "    # Return the model \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic model instance\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Checkpoints During Training\n",
    "Create `ModelCheckpoint` to continually save the model's parameters both during and end of the training. This will help us pick up where we left off if training is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative path for the checkpoint file for the training process\n",
    "checkpoint_path = 'training_1/cp.ckpt'\n",
    "\n",
    "# Finds the working directory's path and appends checkpoint to it\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                save_weights_only=True, \n",
    "                                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 1.1916 - acc: 0.6680 - val_loss: 0.7040 - val_acc: 0.8010\n",
      "\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 288us/step - loss: 0.4399 - acc: 0.8700 - val_loss: 0.5498 - val_acc: 0.8290\n",
      "\n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 279us/step - loss: 0.2942 - acc: 0.9220 - val_loss: 0.4782 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 372us/step - loss: 0.2052 - acc: 0.9570 - val_loss: 0.4499 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 344us/step - loss: 0.1511 - acc: 0.9710 - val_loss: 0.4239 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 304us/step - loss: 0.1101 - acc: 0.9810 - val_loss: 0.4445 - val_acc: 0.8470\n",
      "\n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 339us/step - loss: 0.0829 - acc: 0.9890 - val_loss: 0.4078 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 340us/step - loss: 0.0606 - acc: 0.9940 - val_loss: 0.4161 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 307us/step - loss: 0.0471 - acc: 0.9960 - val_loss: 0.4125 - val_acc: 0.8700\n",
      "\n",
      "Epoch 00009: saving model to training_1/cp.ckpt\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 370us/step - loss: 0.0393 - acc: 1.0000 - val_loss: 0.3960 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb1f2a7438>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with the new callback\n",
    "# May generate warnings\n",
    "model.fit(train_images, train_labels, epochs=10, \n",
    "         validation_data=(test_images, test_labels), \n",
    "         callbacks=[cp_callback])  # Pass the callback to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                  cp.ckpt.index\r\n",
      "cp.ckpt.data-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "# Create a single collection of TF checkpoint files that are updated at the end of each epoch\n",
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untrained Model with Weights\n",
    "We will create a new, untrained model. When restoring a model from weights-only, the model must have the same architecture as the one that was used to save the weights. Weights can be shared even if the new model is a different instance of the same architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 146us/step\n",
      "Untrained model, accuracy; 15.20%\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of a basic model\n",
    "model = create_model()\n",
    "\n",
    "# Evaluate the model prior to using pre-trained weights\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Untrained model, accuracy; {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to using the pre-trained model's weights, we have used randomly initialized weights. This gives a classification accuracy 15.20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 54us/step\n",
      "Restored model, accuracy: 87.50%\n"
     ]
    }
   ],
   "source": [
    "# Load the weights\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Restored model has the same accuracy on the test set as the original model (original model's test set accuracy was actually validation set accuracy since val set was used for test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Callback Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inlcude the epoch in the file name \n",
    "checkpoint_path = 'training_2/cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights every 5 seconds\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    period=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights using the `checkpoint_path` format\n",
    "model.save_weights(checkpoint_path.format(epoch=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: saving model to training_2/cp-0005.ckpt\n",
      "\n",
      "Epoch 00010: saving model to training_2/cp-0010.ckpt\n",
      "\n",
      "Epoch 00015: saving model to training_2/cp-0015.ckpt\n",
      "\n",
      "Epoch 00020: saving model to training_2/cp-0020.ckpt\n",
      "\n",
      "Epoch 00025: saving model to training_2/cp-0025.ckpt\n",
      "\n",
      "Epoch 00030: saving model to training_2/cp-0030.ckpt\n",
      "\n",
      "Epoch 00035: saving model to training_2/cp-0035.ckpt\n",
      "\n",
      "Epoch 00040: saving model to training_2/cp-0040.ckpt\n",
      "\n",
      "Epoch 00045: saving model to training_2/cp-0045.ckpt\n",
      "\n",
      "Epoch 00050: saving model to training_2/cp-0050.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb1cf3fbe0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with the new callback\n",
    "model.fit(train_images,\n",
    "         train_labels,\n",
    "         epochs=50, \n",
    "         callbacks=[cp_callback], \n",
    "         validation_data=(test_images, test_labels), \n",
    "         verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                       cp-0040.ckpt.index\r\n",
      "cp-0030.ckpt.data-00000-of-00001 cp-0045.ckpt.data-00000-of-00001\r\n",
      "cp-0030.ckpt.index               cp-0045.ckpt.index\r\n",
      "cp-0035.ckpt.data-00000-of-00001 cp-0050.ckpt.data-00000-of-00001\r\n",
      "cp-0035.ckpt.index               cp-0050.ckpt.index\r\n",
      "cp-0040.ckpt.data-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "# Examining the latest checkpoint\n",
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_2/cp-0050.ckpt'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference to the latest checkpoint file\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 127us/step\n",
      "Restored model, accuracy: 86.90%\n"
     ]
    }
   ],
   "source": [
    "# Create a new model to test the weights from the latest checkpoint\n",
    "model = create_model()\n",
    "\n",
    "# Load the previously saved weights\n",
    "model.load_weights(latest)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are `ckpt` files?\n",
    "- Weights of the model during (and after) training are stored as a series of checkpoint formatted files.\n",
    "- They contain **only the trained weights of the model** in binary format.\n",
    "- Specifically, consist of \n",
    "    - one or more shards that contain the model's weights.\n",
    "    - An index files that indicates which weights are stored in which shard. \n",
    "- When training a model on a single machine, there will be only one shard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Save Weights\n",
    "\n",
    "So far, we have been using a `keras` callback object for automatically saving our weights at designated, predetermined instances of time.\n",
    "\n",
    "We can also manually save weights at any time using the `save_weights` function. This will also save the weights as a `ckpt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 135us/step\n",
      "Restored model, accuracy: 86.90%\n"
     ]
    }
   ],
   "source": [
    "# Save the weights using the keras `save_weights` function\n",
    "model.save_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "# Create a new model instance\n",
    "model = create_model()\n",
    "\n",
    "# Restore the weights\n",
    "model.load_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "# Evaluate the model \n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Entire Model\n",
    "We save weights when we just want to remember the values of the params in the model, but not necessarily the architecture of the model itself.\n",
    "\n",
    "We use `model.save` to save the entire model (architecture, weights, and training configuration) in a single file or folder. This allows the model to be exported and used elsewhere (on a different device, or a different environment e.g. tensorflow js or tensorflow lite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.1711 - acc: 0.6560\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 288us/step - loss: 0.4256 - acc: 0.8750\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 284us/step - loss: 0.2797 - acc: 0.9320\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 253us/step - loss: 0.2016 - acc: 0.9560\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.1490 - acc: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb1de16d30>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train a new model instance \n",
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file (extension .h5)\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now recreate the exact same model - weight, config, and optimizer - from h5 \n",
    "new_model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "# Show the model's summary\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 74us/step\n",
      "Restored Model, accuracy: 87.20%\n"
     ]
    }
   ],
   "source": [
    "# Check the model's accuracy\n",
    "loss, acc = new_model.evaluate(test_images, test_labels)\n",
    "print(\"Restored Model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
