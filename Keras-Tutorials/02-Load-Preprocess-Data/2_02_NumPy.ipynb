{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_02_NumPy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3hb-6eD82pR",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow Tutorials\n",
        "# Load and Preprocess Data 02 - NumPy\n",
        "\n",
        "Demonstrates how to load data from NumPy arrays into a `tf.data.Dataset` structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6562iJQC9GeK",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bcv7XZo-9Jyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KuMUSXa9NWc",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data From `.npz` File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chjQlxPo9S3j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b1ab24ed-346a-4df3-d28e-c1719b37272c"
      },
      "source": [
        "# URL of dataset from Google's CDN\n",
        "DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'\n",
        "\n",
        "# Download file from CDN to CoLab workspace, remember filepath\n",
        "path = tf.keras.utils.get_file('mnist.npz', DATA_URL)\n",
        "\n",
        "# Open the file at the specified path under the alias `data`\n",
        "with np.load(path) as data:\n",
        "  train_examples = data['x_train']\n",
        "  train_labels = data['y_train']\n",
        "  test_examples = data['x_test']\n",
        "  test_labels = data['y_test']"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgsJsQ_R9ta_",
        "colab_type": "text"
      },
      "source": [
        "## Loading `NumPy` Arrays with `tf.data.Dataset`\n",
        "Pass array of examples and corresponding array of labels as a tuple into the `tf.data.Dataset.from_tensor_slices` to create `tf.data.Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnvguENW92Ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gy29zw9-F3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffling the datasets\n",
        "BATCH_SIZE = 64 \n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlLzM9Xc-yGx",
        "colab_type": "text"
      },
      "source": [
        "Dataset will fill the `buffer` with `buffer_size` elements, then randomly sample elements from this buffer and replace the selected elements with new elements. \n",
        "\n",
        "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0gqnAVm-9Et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# No need to shuffle the test set. Just load the dataset as batches of tensors.\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBuYQI3s_QAx",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiyxJoKo_UU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), \n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), \n",
        "             loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "             metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKvfhY2S_u9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "9d80c7a8-e1f1-4cc3-c653-ff0001831950"
      },
      "source": [
        "# Train\n",
        "model.fit(train_dataset, epochs=10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 2.7838 - sparse_categorical_accuracy: 0.8789\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.5146 - sparse_categorical_accuracy: 0.9304\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3861 - sparse_categorical_accuracy: 0.9480\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3268 - sparse_categorical_accuracy: 0.9556\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2915 - sparse_categorical_accuracy: 0.9620\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2723 - sparse_categorical_accuracy: 0.9661\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2385 - sparse_categorical_accuracy: 0.9686\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2154 - sparse_categorical_accuracy: 0.9726\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2135 - sparse_categorical_accuracy: 0.9747\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2107 - sparse_categorical_accuracy: 0.9761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3ad2df03c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlo_VJMk_1T4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjxRBY2D_3hB",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTeqPjsf_5FO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "25dc8c7c-c6cc-4259-ca6f-08d49b729bb9"
      },
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 2ms/step - loss: 0.6261 - sparse_categorical_accuracy: 0.9605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6260722041657749, 0.9605]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz6gNLF0_91T",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "1. Load .npz files (or any other source of `numpy` arrays).\n",
        "2. Extract train and test examples and labels from the `numpy` array as tensor slices.\n",
        "3. Create a `tf.data.Dataset` object using the `from_tensor_slices` method by passing the numpy arrays of tensors as tuples `(examples, labels)`.\n",
        "4. Can shuffle the dataset using `shuffle`\n",
        "  - Specify a `shuffle_buffer_size`: how many elements the method should select for possible random sampling.\n",
        "  - Specify a `batch_size`: how many elements from the buffer should be randomly sampled. \n",
        "5. Do any further processing/training/evaluation as required using the `tf.data.Dataset`.\n"
      ]
    }
  ]
}