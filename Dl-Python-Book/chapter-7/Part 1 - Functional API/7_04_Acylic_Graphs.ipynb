{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-04: Directed Acyclic Graphs \n",
    "\n",
    "In addition to neural networks with multiple inputs and multiple outputs, the `keras` functional API lets us build networks that resemble acyclic graphs of layers such as Inception modules. \n",
    "\n",
    "The key here is **acyclic**: the network can't be structured such that output tensor of a layer becomes the input tensor of any of the layers that generated the tensor in the first place. Only recurrent connections internal to layers are allowed to have loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Modules\n",
    "- A type of CNN topology.\n",
    "- Consists of several branches that look like smaller independent networks which are then merged or concatenated.\n",
    "- These networks will use 1 x 1, 3 x 3 convolutions and pooling operations. \n",
    "- Simplest Inception Module: 3 - 4 branches of 1 x 1 conv followed by 3 x 3 conv, ending with concatenation. \n",
    "- Inception modules help us learn spatial features (features that share the same tiles or pixels in an image across channels) as well as channel-wise features (features common to all channels in a single tile or pixel). \n",
    "    - More efficient to learn them separately rather than jointly.\n",
    "- 1 x 1 Convolutions are point-wise convolutions\n",
    "    - They compute features that are shared across channels, but not across pixels/space (since looking at one tile at a time).\n",
    "    - Useful if each channel is highly autocorrelated across space, but different channels not correlated with each other. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an Inception Module\n",
    "\n",
    "In this code, `x` is assumed to be a 4-D input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_a = layers.Conv2D(128, 1, activation='relu', strides=2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this branch, striding occurs in the spatial convolutional layer (3 x 3 conv)\n",
    "branch_b = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_b = layers.Conv2D(128, 1, activation='relu', strides=2)(branch_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this branch, striding occurs in the average pooling layer \n",
    "branch_c = layers.AveragePooling2D(3, strides=2)(x)\n",
    "branch_c = layers.Conv2D(128, 3, activation='relu')(branch_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this branch, striding occurs int he final spatial convolutional layer\n",
    "branch_d = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu')(branch_d)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine outputs of all branches to obtain module output\n",
    "output = layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Connections\n",
    "\n",
    "Residual connections are another variant of the acyclic graph topology in which activations from earlier layers (which represent prior information) are reinjected as inputs later in the network.\n",
    "\n",
    "It makes the output of an earlier layer available as an input to a later layer, thus avoiding the problem of vanishing gradients and information bottlenecks. \n",
    "\n",
    "Residual connections will add, rather than concatenate, earlier outputs with later outputs, and therefore assume the earlier output is of the same shape as the activation it is being added to.\n",
    "\n",
    "If this isn't the case, a linear transformation can often convert the earlier activation to the desired shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - Same Activation Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `x` represents a 4-D tensor in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First intermediate Conv2D layer transforms the input x - `same` necessary\n",
    "# because we want the final outputs and residual connection input to have same shape\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "# successive transformations using Conv2D\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "\n",
    "# Add the original input tensor x back to the output features - residual connection\n",
    "y = layers.add([y, x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 - Different Activation Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume again that x is a 4D input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layers will transform the input `x` while keeping input shapes same\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "\n",
    "# The stride here has changed the shape of the activation\n",
    "y = layers.MaxPooling2D(2, strides=2)(y)\n",
    "\n",
    "# Using 1 x 1 convolution to linearly downsample original x tensor to same shape as y\n",
    "residual = layers.Conv2D(128, 1, strides=2, padding='same')(x)\n",
    "\n",
    "# Similar shapes - now add residual tensor back to output features\n",
    "y = layers.add([y, residual])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing Model Weights - Siamese LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a single LSTM layer only once\n",
    "lstm = layers.LSTM(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building left branch of the model: inputs are variable length sequences of vectors of size 128\n",
    "left_input = Input(shape=(None, 128))\n",
    "left_output = lstm(left_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building right branch - calling existing layer instance and reusing its weights\n",
    "right_input = Input(shape=(None, 128))\n",
    "right_output = lstm(right_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a classifier on top of the merged outputs \n",
    "merged = layers.concatenate([left_output, right_output], axis=-1)\n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model linking both input sequences to output predictions\n",
    "model = Model([left_input, right_input], predictions)\n",
    "\n",
    "# Weights of the model will be updated based on both inputs\n",
    "model.fit([left_data, right_data], targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Models as Layers\n",
    "\n",
    "In the previous examples, we used a single `LSTM` layer to transform two different inputs into two different outputs using a common set of weights. \n",
    "\n",
    "The same principle applies to entire models as well: weights of entire models can be reused. \n",
    "\n",
    "This example demonstrates how we can use a pretrained model's weights in combination with video streams from two cameras for depth analysis using a single, pretrained Xception model as the common processing unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate convolutional base of Xception model: comes prepackaged with keras, trained on ImageNet\n",
    "xception_base = applications.Xception(weights=None, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire model is used to process two different streams of input\n",
    "left_input = Input(shape=(250, 250, 3))    # RGB images \n",
    "right_input = Input(shape=(250, 250, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pretrained base to extract features\n",
    "left_features = xception_base(left_input)\n",
    "right_features = xception_base(right_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged features contain information from both visual feeds\n",
    "merged_features = layers.concatenate([left_features, right_features], axis=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
