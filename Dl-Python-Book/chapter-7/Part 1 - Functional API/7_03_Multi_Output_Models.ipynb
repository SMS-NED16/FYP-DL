{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Output Models \n",
    "\n",
    "A multi-output model is one that has more than one output or **head**. For instance, a model that takes the text of a user's social media profile and tries to predict multiple values such as their age, gender, location, income level, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Output Models with `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Problem Constants\n",
    "- `vocabulary_size` is the number of unique words which can occur in the text of any given social media post.\n",
    "- `num_income_groups` is the number of income groups we wish to classify the user into based on their social media post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "num_income_groups = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Input Tensor\n",
    "\n",
    "The `posts_input` is our `Input` tensor. It will consist of variable-length text sequences that have been encoded as a sequence of integers, where each integer corresponds to one of 50,000 words in the `vocabulary`. \n",
    "\n",
    "We also give this `Input` a name - `posts` so that when we're fitting the model later on, we can specify which `numpy` array is to be used as `posts_input` through a dictionary. \n",
    "\n",
    "Later, we embed this 50,000-dimensional vector to a 256-dimensional space using an `Embedding` layer, which accepts the `posts_input` tensor as an argument, processes it, and outputs an `embedded_posts` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_input = Input(shape=(None, ),   # Post can have variable length\n",
    "                   dtype='int32', \n",
    "                   name='posts')      # Naming this input for passing numpy tensor via dict later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_posts = layers.Embedding(256, vocabulary_size)(posts_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the `Input` through `Conv1D` and `MaxPooling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Conv/Pooling Pair\n",
    "x = layers.Conv1D(filters=128, kernel_size=5, activation='relu')(embedded_posts)\n",
    "x = layers.MaxPooling1D(pool_size=5)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Conv/Pooling Set\n",
    "x = layers.Conv1D(filters=256, kernel_size=5, activation='relu')(x)\n",
    "x = layers.Conv1D(filters=256, kernel_size=5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(pool_size=5)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Conv/Pooling Set\n",
    "x = layers.Conv1D(filters=256, kernel_size=5, activation='relu')(x)\n",
    "x = layers.Conv1D(filters=256, kernel_size=5, activation='relu')(x)\n",
    "\n",
    "# This layer requires no specific parameters\n",
    "x = layers.GlobalMaxPooling1D()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layers\n",
    "\n",
    "`x` still refers to the output tensor returned by the `GlobalMaxPooling1D` layer. Instead of passing this to just one layer to predict one of three attributes (`age`, `income`, `gender`), wer are passing it to separate layers for each of these attributes. \n",
    "\n",
    "This means our model has three heads or three outputs. \n",
    "\n",
    "**Every output layer has a name.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age\n",
    "`age_prediction` produces a continuous-valued output, so it is just one unit with the default (`linear`) activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_prediction = layers.Dense(1, name='age')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income \n",
    "\n",
    "We aren't predicting the exact continuous value of income. Rather, we're predicting which of the 10 predefined income classes the user belongs to. For this reason, we use a `softmax` activation in this layer, with one unit per each income group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_prediction = layers.Dense(num_income_groups, \n",
    "                                activation='softmax', \n",
    "                                name='income')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender\n",
    "\n",
    "Because we are not super woke, we're assuming gender to be a binary variable. This means a `sigmoid` layer is the best choice, because it will predict the probability that a given input belongs to the arbitrarily defined positive class representing one of the two genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_prediction = layers.Dense(1, \n",
    "                                 activation='sigmoid', \n",
    "                                 name='gender')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Model\n",
    "\n",
    "Just as before, but this time a the output tensors argument will be a list of the tensors returned by each of our heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(posts_input, [age_prediction, income_prediction, gender_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation\n",
    "\n",
    "We have multiple output values now, which means we must specify a different loss function for each of them. \n",
    "- For instance, **age prediction** is a regression task, so mean squared or absolute error will be a good loss function for this output. \n",
    "- However, **gender prediction** is a binary classification task, so it requires `binary_crossentropy` as a loss function.\n",
    "- The loss function for **income prediction** is categorical crossentropy, because there are 10 different classes to choose from. \n",
    "\n",
    "Because we have named the heads of our network, we can use these names to specify their respective loss functions in the compilation step.\n",
    "\n",
    "However, this is not a requirement. We can still compile the model as long as we pass the loss functions in the same order as the heads were added. Still, it is better to be explicit than implicit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Specifying Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', \n",
    "             loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', \n",
    "             loss={\n",
    "                 'age': 'mse', \n",
    "                 'income': 'categorical_crossentropy', \n",
    "                 'gender': 'binary_crossentropy'\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Weighting\n",
    "\n",
    "Not all losses of a multi-output model will be equal. Some losses will have higher values than others. The gradient descent weight updates will thus be optimised to minimise the loss with the highest value, sometimes at the expense of other losses. \n",
    "\n",
    "This is common if the losses occupy different scales e.g. MSE and crossentropy have an order of magnitude difference between typical values. \n",
    "\n",
    "To remedy this, we can assign weights to each loss in an attempt to 'equalize' the loss values produced for each head.\n",
    "\n",
    "In the following cell, we have 'scaled up' the loss values for `binary_crossentropy`, and scaled down the loss values for `mse`. `categorical_crossentropy` loss values remain unchanged since they are expected to be in roughly the same range as the other two losses after scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Specifying Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without specifying head names\n",
    "model.compile(optimizer='rmsprop', \n",
    "             loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'], \n",
    "             loss_weights=[0.25, 1., 10.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', \n",
    "             loss={\n",
    "                 'age': 'mse', \n",
    "                 'income': 'categorical_crossentropy',\n",
    "                 'gender': 'binary_crossentropy', \n",
    "             }, \n",
    "             loss_weights={\n",
    "                 'age': 0.25, \n",
    "                 'income': 1, \n",
    "                 'gender': 10.,\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "Again, can use the head names to specify the training labels/targets when fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Specifying Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(posts, [age_targets, income_targets, gender_targets], \n",
    "         epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(posts, {\n",
    "    'age': age_targets, \n",
    "    'income': income_targets, \n",
    "    'gender': gender_targets\n",
    "}, epochs=10, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
