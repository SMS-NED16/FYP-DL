{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "\n",
    "# Example 5.3 -  VGG16\n",
    "\n",
    "## Feature Extraction\n",
    "- Each ConvNet-based model will consist of a series of convolution and pooling layers followed by a densely connected classifier.\n",
    "- The conv and pooling layers form the model's convolutional base, which acts as the preprocessor for the densely connected classifier.\n",
    "- It is possible to use the convolutional base of a pretrained model as the first part of a new model, and to use the outputs produced by this pretrained convolutional base to act as the input of a new densely connected classifier that is being trained from scratch.\n",
    "- This is feature extraction: the process of using a pre-trained convolutional base to extract useful features from new samples which can then be used to train a new classifier from scratch.\n",
    "- This is because if the convolutional base was trained on a sufficiently large dataset the spatial hierarchy of featurs it has learned can act as a model of the visual world.\n",
    "\n",
    "## Why Reuse Conv Base and Not Classifier?\n",
    "- Features learnt by the convolutional base are more likely to be generic and therefore reusable. \n",
    "- Feature maps of convnets are presence maps of generic concepts over a picture - these presence maps are likely to be useful regardless of what the picture is.\n",
    "- Classifier's representations will be specific to the set of classes  on which the model was being trained. \n",
    "- Furthermore, information about an object's position or location in an image is lost in a densely connected classifier. \n",
    "- Level of generality depends on the depth of the layer in the model - the deeper the layer, the more specific the feature maps. So if we're using a ConvModel to predict a completely different set of classes than the ones in the data it was trained on, it is better to use the first few layers in the model.\n",
    "- These layers will have learnt generic features that will still be applicable to the new problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the VGG16 model\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      " 1433600/58889256 [..............................] - ETA: 4:42"
     ]
    }
   ],
   "source": [
    "# Instantiate the convolutional base using ImageNet weights\n",
    "# Also specify the input shape for this base \n",
    "conv_base = VGG16(weights='imagenet', \n",
    "                 include_top=False,\n",
    "                 input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Feature Extraction w/o Data Augmentation\n",
    "- Create a subset of the original training data that will be used for the model.\n",
    "- Extract numpy arrays for each image and its labels in the dataset.\n",
    "- Extract features from these images by calling the `predict` method of the `conv_base` model.\n",
    "- This saves the features to a `numpy` array on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the folder with our train/test/validation data from previous example\n",
    "base_dir = '/Users/saads/OneDrive/Desktop/DL-Python/chapter-5/cats_and_dogs_small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for training st\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a data generator to rescale all pixels in all images\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Data generator will iterate over directory and output batches of 20 samples\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    # Output of convent layer will be a batch of 4 x 4 feature maps with 512 filters\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    \n",
    "    # One label for each sample\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    \n",
    "    # Initializing the generator to output batches of images from the specified directory\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,                    # path to the directory that the gen will output images from\n",
    "        target_size=(150, 150),       # size of each image will be 150 x 150px\n",
    "        batch_size=batch_size,        # Images will be output in batches - 20 samples per batch\n",
    "        class_mode='binary',          # Labels will belong to one of two classes for binary crossentropy\n",
    "    )\n",
    "    \n",
    "    # Counter variable that will be used to keep track of batches output by data generator\n",
    "    i = 0\n",
    "    \n",
    "    # For each batch of inputs and labels output by the generator\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        # Pass the inputs through the convolution base and record its output\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        \n",
    "        # Append the output featurs to a list\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        \n",
    "        # Do the same for labels\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        \n",
    "        # Increment batch counter\n",
    "        i += 1\n",
    "        \n",
    "        # if total number of samples output by generator in batches so far\n",
    "        # exceeds the total number of samples to be processed\n",
    "        # Break the control structure - no more output from data generator required\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    \n",
    "    # Return the list of features and labels extracted from the conv base\n",
    "    return features, labels\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features and labels output by the conv base for training data\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "\n",
    "# Get the same for the validation data\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "\n",
    "# Same for test \n",
    "test_features, test_labels = extract_features(test_dir, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping For Input to Classifier\n",
    "The output of the convolutional base will be batches of three dimensional image tensors of shape `(4, 4, 512)` i.e. feature maps of 4 px by 4px containing of activations over 512 different filters.\n",
    "\n",
    "Before this data can be input to a densely connected classifier, it must be reshaped into a 2D tensor of `(samples, 8192)` by flattening each output feature map into a vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densely Connected Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\saads\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\saads\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# input dim vs input shape? dim is dimension of each vector - not specifying sample axis\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "\n",
    "# Half of the outputs of this layer will randomly be changed to 0\n",
    "# This will minimise noise due to random patterns in the layer's activation outputs\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Output layer will again have a single unit that will predict probability\n",
    "# That the output belongs to one of two classes\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5), \n",
    "             loss='binary_crossentropy', \n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "history = model.fit(train_features,\n",
    "                   train_labels,\n",
    "                   epochs=30, \n",
    "                   batch_size=20,\n",
    "                   validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this model will be very fast, because we aren't performing gradient descent on the convolutional base. Those weights are optimized and will not change. We are only optimizing the weights of the densely connected classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from History object\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(val_loss) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy\n",
    "plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b-', label='Validation Accuracy')\n",
    "plt.legend(); plt.grid(True); \n",
    "plt.title('Training and Validation Accuracy - VGG16, No Augmentation');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss - VGG16, No Augmentation')\n",
    "plt.legend(); plt.grid(True); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book's Plots\n",
    "- The plots show that the validation accuracy has increased from ~70% for the baseline model to ~90% for the model with VGG16 base, showing that using the model's convolutional base has indeed improved the model's accuracy.\n",
    "- This means that the VGG16 model was indeed trained on a large enough dataset for the convnet to learn abstract, generic, spatial representations of data (through its weights) that were generic enough for us to be able to extract meaningful features for our specific problem.\n",
    "- However, the plots also show that we are overfitting almost from the beginning\n",
    "    - The training loss decreases exponentially from the beginning, and the validation loss reaaches its minimum value almost at 3 epochs.\n",
    "    - This is why the validation accuracy peaks at around 90% by epoch and then degrades/does not exceed this in future epochs.\n",
    "- This is because we are using very few samples for training and are not using data augmentation.\n",
    "- We are not using data augmentation with this pretrained classifier approach because that would require computing augmented images for each epoch, feeding them into the convnet, computing the convnet's example, and then feeding the resulting data into the classifier. This would be an extremely memory-intensive and slower process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Feature Extraction with Data Augmentation\n",
    "\n",
    "We add a densely connected classifier on top of the VGG16 convolutional base. Then, for each forward pass in each epoch, we compute a new augmented variant of the data - 30 epochs so 30 different augmentations/variants of each image - and pass this image data to the convnet.\n",
    "\n",
    "The convent's weights are frozen, so they aren't optimised during gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the convolutional base to the model\n",
    "model.add(conv_base)\n",
    "\n",
    "# Flatten the model's output for input into the densely connected classifier\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Create a densely connected classifier\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of the model's layers, output shapes, and parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional base has 14.7 million parameters, and the densely connected classifier we hae added on top has an additional 2M parameters. \n",
    "\n",
    "It would be prohibitively expensive to perform gradient descent on the entire 16M parameters. It would also defeat the purpose of this exericse - the whole point is to reuse the features learned by the convolutional base so that we can achieve high classification accuracy without the computational cost of optimising the parameter weights of the convolutional model. \n",
    "\n",
    "To do this, we `freeze` the convolutional base's parameters. This means they will not be optimised during gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the number of trainable weights ', \n",
    "      'before freezing the conv base: ', len(model.trainable_weights))\n",
    "\n",
    "# After the convolutional base of the model has been explicitly frozen\n",
    "conv_base.trainable = False \n",
    "\n",
    "print(\"This is the number of trainable weights \", \n",
    "     \"adter freezing the conv base: \", len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new setup, only the weights from the two densely connected layers that we added will be trained. That is a total of 4 weight tensors (Two per layer: the main weight matrix and the bias vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must compile the model after weight trainability has been modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a training data generator\n",
    "# Now training data can be augmented\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  \n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data can only be scaled, not augmented\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Specifying the flow of data for the training data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, \n",
    "    target_size=(150, 150), \n",
    "    batch_size=20, \n",
    "    class_mode='binary',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Same for the validation generator\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir, \n",
    "    target_size=(150, 150), \n",
    "    batch_size=20, \n",
    "    class_mode='binary',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model - this is important because we have modified weight trainability\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer=optimizers.RMSprop(lr=2e-5), \n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=100, \n",
    "    epochs=30, \n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Model's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-7a736e0a9ab7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Extracting data from History object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Extracting data from History object\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(val_loss) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy\n",
    "plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b-', label='Validation Accuracy')\n",
    "plt.legend(); plt.grid(True); \n",
    "plt.title('Training and Validation Accuracy - VGG16 with Augmentation');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss - VGG16 with Augmentation')\n",
    "plt.legend(); plt.grid(True); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Results\n",
    "- The model's validation accuracy has increased from 90% to 95% or 96%. It also doesn't overfit as quickly as before.\n",
    "- This shows that we have indeed managed to successfully decrease overfitting - at least to some extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('cats_and_dogs_small_vgg16_augment.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "\n",
    "# Section 5.4 - Fine Tuning\n",
    "\n",
    "- Similar (but not identical) to feature extraction.\n",
    "- Still involves reusing a pre-trained convolutional base.\n",
    "- But the top layers of a frozen model base are unfrozen and trained with the densely connected classifier.\n",
    "- Adjusts the more abstract representations of the model being reused by reoptimizing the weights of the top layers in the convolutional base.\n",
    "- The layers near the beginning of the convolutional model learn very generic features such as edges, and successive layers form an increasingly complex spatial hierarchy: they will learn increasingly abstract features made up more generic, lower-level features.\n",
    "\n",
    "### Steps in Fine-Tuning\n",
    "1. Add custom network on top of an already-trained base network.\n",
    "2. Freeze the base network.\n",
    "3. Train the part of the network that you added.\n",
    "4. Unfreeze some layers in the base network.\n",
    "5. Jointly train both these layers and the part you added. \n",
    "\n",
    "\n",
    "### Training Sequence\n",
    "- Must train the densely connected classifier on top of the convolutional base before the convolutional layers can be fine-tuned.\n",
    "- This is because if the model on the top of the base has not already been optimized, the error signal propagating through the network during training will be too large. \n",
    "- This will cause representations previously learned by the layers beign fine-tuned to be destroyed.\n",
    "- **We don't want to completely reinitialize the layers in the top** of the convolutional base; we just want to fine-tune them i.e. nudge their weights in the right direction ever so slightly.\n",
    "\n",
    "### Why not fine-tune more layers?\n",
    "- Earlier layers in the convolutional base encode more generic, reusable features whereas layers higher up encode more specialized features. It makes more sense to fine-tune the specialized features as they are the ones that need to be repurposed to a new problem. \n",
    "- The more layers we fine-tune, the more parameter we will have to train, and the more we will be at the risk of overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-6a41904b84a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Check its shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mconv_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'conv_base' is not defined"
     ]
    }
   ],
   "source": [
    "# Check its shape\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-3f4d67665317>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# First configure the convolutional base to modify its weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mconv_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Flag that will be changed to true as soon as we enter block 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conv_base' is not defined"
     ]
    }
   ],
   "source": [
    "# Freezing all layers upto a specific one\n",
    "\n",
    "# First configure the convolutional base to modify its weights\n",
    "conv_base.trainable = True\n",
    "\n",
    "# Flag that will be changed to true as soon as we enter block 5\n",
    "set_trainable = False \n",
    "\n",
    "# Parse all the layers in the base \n",
    "for layer in conv_base.layers:\n",
    "    # As soon as you encounter the first layer in block 5\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True                 # flag state changes\n",
    "    \n",
    "    # If the flag is true, the current layer's `trainable` parameter is also true\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    # Otherwise, the layer cannot be modified\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be using `RMSprop` optimizer with a very low learning rate because we do not want the gradient descent updates to be too large - we want to limit the magnitudes of the modifications we make to the representations of the three layers we are fine tuning. Updates that are too large may harm these representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer=optimizers.RMSprop(lr=1e-5), \n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=100,\n",
    "    epochs=100,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Loss/Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curves(points, factor=0.8):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from History object\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(val_loss) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy\n",
    "plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b-', label='Validation Accuracy')\n",
    "plt.legend(); plt.grid(True); \n",
    "plt.title('Training and Validation Accuracy - VGG16 with Augmentation');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss - VGG16 with Augmentation')\n",
    "plt.legend(); plt.grid(True); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, \n",
    "        smooth_curve(acc), 'bo', label='Smoothed Training Accuracy')\n",
    "plt.plot(epochs, \n",
    "        smooth_curve(val_acc), 'b-', label='Smoothed Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy - Fine Tuned Model')\n",
    "plt.legend(); plt.grid(True); plt.xlabel('Epochs'); plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy improves even though loss does not because while loss is displayed as a point-wise average, what matters for accuracy is the distribution of the loss values and not their average.\n",
    "\n",
    "Accuracy is the result of binary thresholding of the class probability predicted by the model so it is the distribution of the loss values that matters, and not their average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir, \n",
    "    target_size=(150, 150), \n",
    "    batch_size=20,\n",
    "    class_mode'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate_Generator(test_generator, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Accuracy: ', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
