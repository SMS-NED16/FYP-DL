{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "# 5.5 - Heatmaps of Class Activation\n",
    "\n",
    "- A visualization technique which is useful for understanding which parts of a specific image led a convnet to its final classification.\n",
    "- Especially relevant in the case of a classification mistake. \n",
    "- Can also be used to identify specific objects within an image. \n",
    "\n",
    "## Class Activation Maps (CAMs)\n",
    "- Producing heatmaps of class activation over input images.\n",
    "- A class activation map is a 2D grid of scores associated with respect to a specific output class that is computed for **every location in any input image**.\n",
    "- It indicates how important each location (pixel) in the image is for the convnet to classify the image in a specific class.\n",
    "- We're using Grad-CAM: a technique that\n",
    "    - Takes the output feature map of a convolution layer for a given input.\n",
    "    - Weighs every channel in that feature map by the gradient of the class with respect to the channel.\n",
    "- Intuitively\n",
    "    - We're computing the activation of an image with respect to a specific channel.\n",
    "    - And then computing the activation of that channel with respect to a specific class.\n",
    "    - The result of this cascaded series of operations ultimately results int he activation of an image with respect to a specific class.\n",
    "- Really iffy mathematical notation for my own understanding\n",
    "$$\\frac{\\delta[image activation]}{\\delta[channel]} \\times \\frac{\\delta[channel]}{\\delta[class]} = \\frac{\\delta[image activation]}{\\delta[class]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - African Elephants and VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\saads\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of 64373286793e3c8b2b4e3219cbf3544b so we will re-download the data.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "  8986624/553467096 [..............................] - ETA: 50:45"
     ]
    }
   ],
   "source": [
    "# This time we're including the densely connected classifier\n",
    "model = VGG16(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Image\n",
    "\n",
    "Need to preprocess the image by\n",
    "- load the image\n",
    "- resize to 244 x 244 pixels\n",
    "- convert it to a `numpy` `float32` array\n",
    "- apply VGG16's built-in preprocessing rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing an input image for VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the path of the Elephant's image\n",
    "img_path = '/Users/saads/OneDrive/Desktop/DL-Python/chapter-5/cc_elephant.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image as a Python object (Python Imaging Library of size (224, 224))\n",
    "img = image.load_img(img_path, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to an array of shape (224, 224, 3)\n",
    "x = image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds a dimension to transform the array into a batch of size (1, 224, 224, 3)\n",
    "x = np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the batch with VGG's built-in rules for channel-wise color normalization\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Network Predictions\n",
    "Run the pretrained network on the image and decode its prediction vector back to human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the prediction accuracy and class name for the top three guesses\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# African Elephant entry was found to be at index 386 using argmax \n",
    "african_elephant_output = model.output[:, 386]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output feature map of the block5_conv3 layer - the last layer in the VGG16 model\n",
    "last_conv_layer = model.get_layer('block5_conv3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of the African Elephant class with regard to the output feature map of block5_conv3\n",
    "grads = K.gradients(african_elephant_output, \n",
    "                   last_conv_layer.output[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector of shape (512, ) where each entry is the mean intensity\n",
    "# of the gradient over a specific feature-map channel\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplieseach channel in the feature-map array by\n",
    "# \"how important this channel is\" w.r.t the `elephant` class\n",
    "for i in range(512):\n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the values of the quantities we just defined\n",
    "# pooled_grads and the output feature map of the block5_conv3 \n",
    "# given the sample image\n",
    "iterate = K.function([model.input], \n",
    "                    [pooled_grads, last_conv_layer.output[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_grads_value, conv_layer_output_value = iterate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel-wise mean of the resulting feature map is the heatmap of class activation\n",
    "heatmap = np.mean(conv_layer_output_value, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "plt.matshow(heatmap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
