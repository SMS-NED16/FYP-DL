{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "# 6.1 - One-Hot Encoding\n",
    "\n",
    "- A tokenization technique for text-based data.\n",
    "- Tokenization is the process of breaking text-based data into individual units such as words or characters (called tokens) that can then be encoded as vectors.\n",
    "- Tokenization allows us to convert text-based data into numeric tensors which can then be passed to a Deep Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Level One-Hot Encoding\n",
    "Every **word** in the sentence is considered an individual building block for text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples represents the data that we will be tokenizing and then vectorizing through one-hot encoding. In this case, each sample is a sentence, but a sample can also be an entire document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat', 'The dog ate my homework']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Samples\n",
    "We will create a dictionary of called `token_index` which will map each unique word in all our samples to a unique index or identifier. The key is the word itself and the value is an index number.\n",
    "\n",
    "We do this by parsing each sample in the `samples` list, and splitting it into individual words. IRL we would also remove punctuation marks and special characters from the samples.\n",
    "\n",
    "The result of `split()` on a single sample is a list of words. We then check if each word in the list of words is already present in the `token_index` dictionary. If this is not the case, we add the word as a key to the dictionary with its value set to the total number of unique words in the total words added to the dictionary so far. We add 1 to this index because the index `0` is usually not assigned to any word. It is reserved, possibly for characters such as space or for invalid keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_index is a dictionary mapping words to index numbers\n",
    "token_index = {}\n",
    "\n",
    "# For every sample in the corpus/collection of samples/documents\n",
    "for sample in samples:\n",
    "    for word in sample.split(): \n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When parsing a sample, we will limit the program to a fixed number of words to tokenize. So the first 10 words in each sample will be tokenized. The remainining words will not. This keeps the word index dictionary's size manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new three-dimensional vector to store the results of tokenizing the samples.\n",
    "- Axis 0 - The batch or sample axis: one dimension per sample to be tokenized/vectorized.\n",
    "- Axis 1 - `max_length` - the maximum number of words that will be considered for tokenization for a sample.\n",
    "- Axis 2 - `max(token_index.values()) + 1` - For each word to be tokenized, we will create an `n + 1` dimensional vector, where `n` is the number of unique words in our dictionary. An additional dimension is added to account for `0` - the index that is not assigned to any word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros(shape=(len(samples), \n",
    "                         max_length, \n",
    "                         max(token_index.values()) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in the `samples` array is first split into a list of individual words. Then `enumerate` is used to assign an index to each word in the list of words (up to the maximum number of words to be tokenized). The `get` method is used to look up the word in the dictionary of unique words and get its `index` value. Then, for each word, the value in the `index` column is set to 1. (a `float32` value, not an `int` value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample in the collection of documents\n",
    "for i, sample in enumerate(samples):\n",
    "    # For every word in each sample up to the defined number of words\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The': 1,\n",
       " 'cat': 2,\n",
       " 'sat': 3,\n",
       " 'on': 4,\n",
       " 'the': 5,\n",
       " 'mat': 6,\n",
       " 'dog': 7,\n",
       " 'ate': 8,\n",
       " 'my': 9,\n",
       " 'homework': 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the dictionary of unique words in all samples\n",
    "# Since we aren't converting words to lowercase, `The`, and `the` are different\n",
    "token_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the `results` array is `(2, 10, 11)`. 2 samples. In the second axis, there is one dimension for each potential word that will be tokenized in each sample (10). In the third axis, there is one dimension per each unique word in the dictionary (not accounting for case sensitivity) and an additional dimension for the 0th index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the one-hot encoded version of the first sample to confirm manually encoded results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`string` library must be imported because we need to use the `printable` attribute to extract all ASCII characters from the string. This is a way of dropping invalid (non-alphanumeric) characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples are still the same as the ones used in word-based encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat', 'The dog ate my homework']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a string that contains all valid alphanumeric characters. This will be used to check for characters to extract from the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = string.printable # All printable ASCII characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary where the keys will be the index of the characters and the values will be the actual character each index encodes. \n",
    "\n",
    "The `zip` function is used to combine two list of the same length into a single data structure where each element is a tuple consisting of the elements at the same index in each list. In this way we're passing a list of tuples to the dictionary in the form `(index, character)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_index = dict(zip(range(1, len(characters) + 1), characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit the tokenizer to the first 50 characters in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `numpy` array that of all zeros with the following dimensions\n",
    "- Axis 0 is still the batch axis and has one dimension per each sample to be encoded.\n",
    "- Axis 1 has one dimension per each character in the sample to be encoded. Limited to 50 characters.\n",
    "- Axis 2 has one dimension per each unique character present in the dictionary of characters along with an additional dimension for the 0 index. This time we're using the `keys` rather than the `values` of the dictionary to determine the dimensions because the `keys` stores the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((len(samples), max_length, \n",
    "                    max(token_index.keys()) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly different logic. Instead of splitting each sample into individual words at the space ` ` character, we're using `enumerate` to convert it to a list of characters, each with its own index. This index is then used to find the location in the one-hot vector for each character that should be set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `keras` One-Hot Encoding\n",
    "- Don't reinvent the wheel.\n",
    "- Use `keras`'s built in one-hot encoding functionality. \n",
    "- These utilities will strip away all special characters from strings and account for only the first `N` most commonly occurring words.\n",
    "- Using a finite number of commonly occurring words helps us avoid dealing with evry large input vector spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat', 'The dog ate my homework']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer configured to take into account only the 1000 most common words\n",
    "tokenizer = Tokenizer(num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the word index - turns strings into list of integer indices\n",
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn strings into lists of integer indices\n",
    "sequences = tokenizer.texts_to_sequences(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could also directly get 1-hot binary representations\n",
    "# Other vectorization modes than 1-Hot also supported\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Recovering the word index that was computed\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.'%len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Hashing\n",
    "\n",
    "When there are far too many words for us to assign an explicit word index to, we can use a variant of the one-hot encoding scheme called **one-hot hashing**. This involves assigning a random index to a word in a vector of fixed size through a hash function.\n",
    "\n",
    "The major advantage of this method is that it allows for online encoding of word vectors: all words do not need to be encoding in one go (as is the case in using conventional, index-based one-hot encoding). \n",
    "\n",
    "Also, this scheme does away with maintaining an explicit word index, which saves memory. \n",
    "\n",
    "The drawback of this approach is hash collisions - when the dimensionality of the vectors used to store the words is close to/less than the number of unique words, the hash function may assign the same index to more than one word, which will confuse the ML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores words as vectors of size 1000. If we have close to 1000 words\n",
    "# (or more) we are more likely tosee hash collisions, which will\n",
    "# decrease the accuacy of the encoding method. \n",
    "dimensionality = 1000\n",
    "max_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((len(samples), \n",
    "                    max_length, \n",
    "                    dimensionality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        # Hash to a random index between 0 and 1000\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
