{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "# 6.2 - Word Embeddings\n",
    "## What?\n",
    "- **One-hot encoding** or one-hot hashing creates word vectors that are\n",
    "    - High dimensional (> 20k tokens so 20k dimensions)\n",
    "    - Sparse: All elements are 0, only the index corresponding to a specific word is 1. \n",
    "    - Binary: The value of any element in a one-hot vector is either 1 or 0.\n",
    "    - Explicilty defined or hardcoded, not learned through a machine learning algorithm. \n",
    "- **Word embeddings** are vectors of words that are\n",
    "    - Low-dimensional: Word-embedding vectors are 256, 512, or 1024 dimensional for vocabularies of 20k words or more. They pack a lot more words/information in lower dimensions.\n",
    "    - Floating point: Not binary. Each element can be a floating point number. Makes them amenable to tensor models.\n",
    "    - Learned from data.\n",
    "    \n",
    "Two ways to obtain word embeddings\n",
    "- Learn word embeddings jointly with training your model.\n",
    "- Use preconfigured or pretrained word embeddings that were computed using a different machine learning task than the one you are trying to solve.\n",
    "\n",
    "### TLDR: Word Embeddings\n",
    "- Word embeddings map words in the vocabulary for a specific machine learning task to a geometric space.\n",
    "- The separation between individual words in this vector space (such as L2 norm) is analogous to the semantic difference between the words.\n",
    "- Embedding vectors represent transformations that express relationships between words in a word embedding vector space. \n",
    "- E.g. a `female` word embedding vector, when added to the `King` in a word embedding vector space, will form `queen`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Word Embeddings\n",
    "### Why?\n",
    "- No universal word embedding space that can be used for all languages and all possible NLP tasks.\n",
    "- All languages are different and are not isomorphic: relationships in one language's word-embedding space may not be transferable to that of another language.\n",
    "- In fact, even within the same language, word embedding spaces differ from one application to another.\n",
    "- So it's a good idea to learn a word-embedding space from scratch when working on a new ML problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding layer is best understood as a dictionary that mps integer indices (which represent specific words) to dense vectors. It takes integers (indices) as inputs, looks up these integrs in an internal dictionary, and returns the associated vectors as ouput. \n",
    "\n",
    "In this case, the Embedding layer recieves an input tensor containing up to 1000 word tokens (indices representing up to 1000 words in the total vocabulary for our samples) and will return the vectors representing each word as a 64 dimensional word-embedding space vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arg 1: Number of possible tokens - 1000 = 1 + maximum word index\n",
    "# Arg 2: Dimensionality - 64 - Ou\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Layer will take as input a 2D tensor of integers of shape `(samples, sequence_length)`. Here, each entry represents a single sample and each sample is a sequence of integers. All sample vectors must have the same length, so individual sample vectors may have to be zero-padded. \n",
    "\n",
    "The output will be a 3D `float32` tensor of shape `(samples, sequence_length, embedding_dimensionality)` where each sample is will consist of the sequence of words, and each word will be represented as an `N` dimensional vector, where `N` is the dimensionality of the word-embedding vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Embedding for `imdb`\n",
    "\n",
    "1. Prepare data by tokenizing it to a sequence of integers.\n",
    "2. Restrict the movie reviews to the top 10k most common words.\n",
    "3. Parse only the first 20 words in each review for tokenizing.\n",
    "4. Feed each sequence of integers to the Embedding layer which will convert each word (integer) to an 8-dimensional vector. \n",
    "5. Flatten the tensor to 2D.\n",
    "6. Train a single `Dense` layer on top for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words to consider as features\n",
    "max_features = 10000\n",
    "\n",
    "# Cuts off the text after this number of words\n",
    "maxlen = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading IMDB dataset from a pickle file\n",
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "# Data is loaded as lists of integers\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesing\n",
    " Turn the list of training and test integers into 2D integer tensor of shape `(samples, maxlen)`. This means any reviews that have less than 20 words will have to be zero-padded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns list of training integers into 2D integer tensor\n",
    "# shape (samples, maxlen) - 0 padding for reviews that are less than 20 words\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the test samples\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `Embedding` Layer and Classifier on IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer will accept inputs with upto 10k different words and encodes each word as an 8 dimensional word-embedding vector. The `input_length` argument specifies the dimension of each input to the layer along the samples axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifies maximum input length to Embeddinag layer\n",
    "# So embedded inputs can later be flattened\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 3D tensor of embeddings into a 2D tensor of shape\n",
    "# (samples, maxlen * 8)\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a classifier on top \n",
    "# But this will treat each word in the sample separately\n",
    "# Without considering inter-word relationships and structure\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\saads\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 3s 169us/sample - loss: 0.6633 - acc: 0.6349 - val_loss: 0.6048 - val_acc: 0.7014\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 2s 107us/sample - loss: 0.5291 - acc: 0.7531 - val_loss: 0.5196 - val_acc: 0.7314\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 2s 108us/sample - loss: 0.4571 - acc: 0.7893 - val_loss: 0.5000 - val_acc: 0.7474\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 2s 117us/sample - loss: 0.4228 - acc: 0.8073 - val_loss: 0.4948 - val_acc: 0.7516\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 2s 120us/sample - loss: 0.3995 - acc: 0.8196 - val_loss: 0.4943 - val_acc: 0.7534\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 2s 109us/sample - loss: 0.3802 - acc: 0.8313 - val_loss: 0.4976 - val_acc: 0.7546\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 2s 110us/sample - loss: 0.3632 - acc: 0.8420 - val_loss: 0.5018 - val_acc: 0.7558\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 2s 110us/sample - loss: 0.3461 - acc: 0.8514 - val_loss: 0.5077 - val_acc: 0.7538\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 2s 116us/sample - loss: 0.3295 - acc: 0.8597 - val_loss: 0.5129 - val_acc: 0.7528\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 3s 161us/sample - loss: 0.3123 - acc: 0.8709 - val_loss: 0.5194 - val_acc: 0.7510\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "                   epochs=10, batch_size=32, \n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a validation accuracy of 76%, which is good considering that we were only using the first 20 words in each review for learning word embeddings. \n",
    "\n",
    "The use of a densely connected classifier without an RNN or Conv1D layer is not the optimal way of learning word embeddings. This is because this architecture will only allow the NN to focus on individual words in the input sequences without considering inter-word relationshops and sentence structure.\n",
    "\n",
    "So this network would be unable to differentiate between \"This movie is a bomb\" and \"This move is the bomb\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word Embeddings\n",
    "\n",
    "To integrate pretrained word embeddings with our model, we must use the original IMDB data. This is because the IMDB data accessible through built-in `keras` functionality already has associations/its own encoding. We want the machine learnign algorithm to assign these encodings from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to original IMDB dataset on Drive\n",
    "imdb_dir = '/Users/saads/OneDrive/Desktop/DL-Python-Repo/FYP-DL/Dl-Python-Book/chapter-6/aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to training directory - renamed to test in the new download\n",
    "train_dir = os.path.join(imdb_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists to store class and extracted text from all reviews\n",
    "labels = []\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = os.listdir(os.path.join(train_dir, 'pos'))\n",
    "neg_files = os.listdir(os.path.join(train_dir, 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.txt'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_files[0][-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all .txt files in the `neg` and `pos` subdirectories of \n",
    "# the training set. Read all reviews and add their text and labels to the \n",
    "# appropriate lists. \n",
    "for label_type in ['neg', 'pos']:\n",
    "    # Parse the negative and positive class directories in turn\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    \n",
    "    # For every file in the subdirectory\n",
    "    for fname in os.listdir(dir_name):\n",
    "        # Check if it's a text file. DON'T FORGET CLOSING COLON FOR THE LIST SLICING\n",
    "        if fname[-4:] == '.txt':\n",
    "            # Open it, read its contents, append them to the texts list\n",
    "            # Must specify UTF-8 encoding, otherwise Python has issues reading from file\n",
    "            f = open(os.path.join(dir_name, fname), encoding='utf-8')\n",
    "            texts.append(f.read())\n",
    "            \n",
    "            # Close the filestream\n",
    "            f.close()\n",
    "            \n",
    "            # For each review added to the texts list, also add\n",
    "            # 1 or 0 to the labels category on the corresponding index\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)  # negative class\n",
    "            else:\n",
    "                labels.append(1)  # positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Data\n",
    "\n",
    "The data is now ready to be tokenized i.e. converted from string to a series of integer indices, each of which represents a single word in the vocabulary.\n",
    "\n",
    "A condition where pre-trained word embeddings are practically useful is when we have very little training data available to learn our own embeddings. To simulate this condition, we will limit ourselves to using only 200 samples fromn the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuts off reviews after 100 words\n",
    "maxlen = 100\n",
    "\n",
    "# Trains on only 200 samples - simulating limited training data \n",
    "training_samples = 200\n",
    "\n",
    "# Validate on 10k samples\n",
    "validation_samples = 10000\n",
    "\n",
    "# Considers only the top 10k words in the dataset\n",
    "max_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a keras tokenizer for 10k words\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "# Tokenize all words in the training data\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert the samples to sequences of tokens\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that maps each word in the vocabulary to an index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72745 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor:  (22455, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data tensor: \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor:  (22455, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of label tensor: \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and validation set\n",
    "# But first shuffle the data b/c we're starting with data\n",
    "# in which samples are ordered (all negative first, then all positive)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:training_samples]\n",
    "y_train = data[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = data[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 100)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = '/Users/saads/OneDrive/Desktop/DL-Python-Repo/FYP-DL/Dl-Python-Book/chapter-6/glove.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Found %s word vectors.' %len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in the embedding index will be all zeros\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the GloVe Embeddings n the Model\n",
    "\n",
    "Weight matrix of the embedding layer is a 2D float matrix where each entry `i` is the word vector meant to be associated with the index `i`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "\n",
    "# Embedding layer's weights are not trainable\n",
    "# Want to reuse preconfigured features in the word-embedding\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model.compile(optimizer='rmsprop', \n",
    "             loss='binary_crossentropy', \n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "                   epochs=10, batch_size=32, \n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Traning Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b-', label='Validation Accuracy')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.grid(True);\n",
    "plt.legend(); plt.title('GloVe Embeddings - Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b-', label='Validation Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.grid(True);\n",
    "plt.legend(); plt.title('GloVe Embeddings - Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training w/o GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, \n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "labels = []\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name))\n",
    "    if fname[-4:] == '.txt':\n",
    "        f = open(os.path.join(dir_name, fname))\n",
    "        texts.append(f.read())\n",
    "        f.close()\n",
    "        \n",
    "        if label_type == 'neg':\n",
    "            labels.append(0)\n",
    "            \n",
    "        else:\n",
    "            labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
