{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python \n",
    "# 6.3 - Recurrent Neural Networks\n",
    "\n",
    "All neural networks we have seen so far do not maintain **state** between processing inputs. This means that each input is processed independently, and previous inputs do not affect the gradient updates to the weights of a model for the current input. \n",
    "\n",
    "To get around this problem, we fed each sequence of inputs to the NN in one go. E.g. each IMDb review can be considered as a sequence of words, but the NN will receive the entire review in one go instead of parsing it word by word (or word-vector by word-vector).\n",
    "\n",
    "RNN **maintains a state between inputs** but the **state is reset between samples**. The output of the current input will act as a feedback input along with the next input for a given sample. In this case, each sample is a sequence of inputs. \n",
    "\n",
    "This is called the **forward pass**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass of an RNN\n",
    "- RNN takes as input a sequence of vectors (encoded as 2D `numpy` tensors). \n",
    "- Loops over timesteps and after each timestep it considers its urrent state at `t` and the input at `t` (of shape `input_features`) and combines them to obtain the output at `t`.\n",
    "- Then sets the state for the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The state at t - initially this will be 0\n",
    "state_t = 0 \n",
    "\n",
    "# Iterate over sequence elements\n",
    "for input_t in input_seqence:\n",
    "    # Uses the current input and state to compute output\n",
    "    output_t = f(input_t, state_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the transformation of the current input and the current state to produce the current output uses kernel matrices `W` and `U` along with a bias vector `b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t = 0\n",
    "for input_t in input_sequence:\n",
    "    output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
    "    state_t = output_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a complete naive `numpy` implementation of an RNN's update based on an input and state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 100\n",
    "\n",
    "# Dimensionality of the input feature space\n",
    "input_features = 32\n",
    "\n",
    "# Dimensionality of the output feature space\n",
    "output_features = 64\n",
    "\n",
    "# Input data is random noise for the sake of this example\n",
    "# At each of the timesteps, we have the same number of features\n",
    "inputs = np.random.random((timesteps, input_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial state (prior to any training) will be all zeros\n",
    "state_t = np.zeros((output_features,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining random weight matrices\n",
    "W = np.random.random((output_features, input_features))\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the outputs produced at each timestep\n",
    "successive_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `input_t` is a vector of shape (input_features, )\n",
    "for input_t in inputs:\n",
    "    # Using the hyperbolic tangent activation function\n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
    "    \n",
    "    # Append this outout to the list of computed outputs\n",
    "    successive_outputs.append(output_t)\n",
    "    \n",
    "    # The state for the next timestep will be the current output\n",
    "    state_t = output_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final output is a 2D tensor of shape `(timesteps, output_features)`\n",
    "final_output_sequence = np.concatenate(successive_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest variant of an RNN. Many other RNN architectures exist, and all of them have different ways of using the current output or state as an input in computing the next state. \n",
    "\n",
    "The output of the RNN is a tensor of shape `(timesteps, output_features)`. The output features for timestep `t` will contain information about outputs of **all timesteps up to and including `t`**. This means we don't often need the entire output tensor for an RNN, and are mostly interested in the final O/P."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN and Return Sequences\n",
    "When using the `keras` RNN layer, we have the option of specifying whether we want a 3D output tensor which contains all output features for all timesteps for all samples in the batch, or whether we want only the last output for each input sequence. \n",
    "\n",
    "These are controlled by the `return_sequences` constructor argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns only the Last Output for Each Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding layer will accept samples containing upto 10000\n",
    "# different words and will encode each word as a 32-dimensional vector\n",
    "model.add(Embedding(10000, 32))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(SimpleRNN(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 322,080\n",
      "Trainable params: 322,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns the Full State Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, None, 32)          2080      \n",
      "=================================================================\n",
      "Total params: 322,080\n",
      "Trainable params: 322,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Multiple Recurrent Layers\n",
    "Stacking multiple recurrent layers in sequence increases the representational power of a network. To do this, all intermediate recurrent layers must return the full sequences. Can't just return the last output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "\n",
    "# for the last recurrent layer, we need not return outputs for all timesteps\n",
    "model.add(SimpleRNN(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_7 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_8 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_9 (SimpleRNN)     (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 328,320\n",
      "Trainable params: 328,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
