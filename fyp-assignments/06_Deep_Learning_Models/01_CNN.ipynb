{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FYP-Data-Transformation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24S1S8ey-op3",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader for CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8dDkx-gLQr8",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc6ip6GnLSb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "# Plotting imports\n",
        "from seaborn import distplot\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib import style\n",
        "style.use('ggplot')\n",
        "\n",
        "# Scikit-Learn imports\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2PLmXURLgH_",
        "colab_type": "text"
      },
      "source": [
        "# Loading Data\n",
        "\n",
        "**What is being loaded?**: `outliersRemoved.csv` \n",
        "This data has is sorted, has no missing values and has no outliers. However, it is **not scaled**.\n",
        "\n",
        "You will scale the data as you wish by running the cells in the **Scaling** section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK71t3SELn0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = '/content/drive/My Drive/EE 16-17 FYP DL Energy Theft Detection/Data/full-processing-data/outliersRemoved.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QexEY55KMMCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_data = pd.read_csv(filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oOq2sliMipH",
        "colab_type": "code",
        "outputId": "2f460893-44f6-434f-86c1-45f733e3c580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "# Did it work?\n",
        "raw_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CONS_NO</th>\n",
              "      <th>FLAG</th>\n",
              "      <th>2014-01-01</th>\n",
              "      <th>2014-01-02</th>\n",
              "      <th>2014-01-03</th>\n",
              "      <th>2014-01-04</th>\n",
              "      <th>2014-01-05</th>\n",
              "      <th>2014-01-06</th>\n",
              "      <th>2014-01-07</th>\n",
              "      <th>2014-01-08</th>\n",
              "      <th>2014-01-09</th>\n",
              "      <th>2014-01-10</th>\n",
              "      <th>2014-01-11</th>\n",
              "      <th>2014-01-12</th>\n",
              "      <th>2014-01-13</th>\n",
              "      <th>2014-01-14</th>\n",
              "      <th>2014-01-15</th>\n",
              "      <th>2014-01-16</th>\n",
              "      <th>2014-01-17</th>\n",
              "      <th>2014-01-18</th>\n",
              "      <th>2014-01-19</th>\n",
              "      <th>2014-01-20</th>\n",
              "      <th>2014-01-21</th>\n",
              "      <th>2014-01-22</th>\n",
              "      <th>2014-01-23</th>\n",
              "      <th>2014-01-24</th>\n",
              "      <th>2014-01-25</th>\n",
              "      <th>2014-01-26</th>\n",
              "      <th>2014-01-27</th>\n",
              "      <th>2014-01-28</th>\n",
              "      <th>2014-01-29</th>\n",
              "      <th>2014-01-30</th>\n",
              "      <th>2014-01-31</th>\n",
              "      <th>2014-02-01</th>\n",
              "      <th>2014-02-02</th>\n",
              "      <th>2014-02-03</th>\n",
              "      <th>2014-02-04</th>\n",
              "      <th>2014-02-05</th>\n",
              "      <th>2014-02-06</th>\n",
              "      <th>2014-02-07</th>\n",
              "      <th>...</th>\n",
              "      <th>2016-09-22</th>\n",
              "      <th>2016-09-23</th>\n",
              "      <th>2016-09-24</th>\n",
              "      <th>2016-09-25</th>\n",
              "      <th>2016-09-26</th>\n",
              "      <th>2016-09-27</th>\n",
              "      <th>2016-09-28</th>\n",
              "      <th>2016-09-29</th>\n",
              "      <th>2016-09-30</th>\n",
              "      <th>2016-10-01</th>\n",
              "      <th>2016-10-02</th>\n",
              "      <th>2016-10-03</th>\n",
              "      <th>2016-10-04</th>\n",
              "      <th>2016-10-05</th>\n",
              "      <th>2016-10-06</th>\n",
              "      <th>2016-10-07</th>\n",
              "      <th>2016-10-08</th>\n",
              "      <th>2016-10-09</th>\n",
              "      <th>2016-10-10</th>\n",
              "      <th>2016-10-11</th>\n",
              "      <th>2016-10-12</th>\n",
              "      <th>2016-10-13</th>\n",
              "      <th>2016-10-14</th>\n",
              "      <th>2016-10-15</th>\n",
              "      <th>2016-10-16</th>\n",
              "      <th>2016-10-17</th>\n",
              "      <th>2016-10-18</th>\n",
              "      <th>2016-10-19</th>\n",
              "      <th>2016-10-20</th>\n",
              "      <th>2016-10-21</th>\n",
              "      <th>2016-10-22</th>\n",
              "      <th>2016-10-23</th>\n",
              "      <th>2016-10-24</th>\n",
              "      <th>2016-10-25</th>\n",
              "      <th>2016-10-26</th>\n",
              "      <th>2016-10-27</th>\n",
              "      <th>2016-10-28</th>\n",
              "      <th>2016-10-29</th>\n",
              "      <th>2016-10-30</th>\n",
              "      <th>2016-10-31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0387DD8A07E07FDA6271170F86AD9151</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>8.56</td>\n",
              "      <td>10.48</td>\n",
              "      <td>8.20</td>\n",
              "      <td>12.53</td>\n",
              "      <td>7.90</td>\n",
              "      <td>8.58</td>\n",
              "      <td>10.12</td>\n",
              "      <td>9.96</td>\n",
              "      <td>7.60</td>\n",
              "      <td>18.19</td>\n",
              "      <td>10.93</td>\n",
              "      <td>11.41</td>\n",
              "      <td>19.92</td>\n",
              "      <td>10.70</td>\n",
              "      <td>9.20</td>\n",
              "      <td>6.77</td>\n",
              "      <td>6.760000</td>\n",
              "      <td>6.150000</td>\n",
              "      <td>6.500000</td>\n",
              "      <td>8.080000</td>\n",
              "      <td>7.97</td>\n",
              "      <td>8.810000</td>\n",
              "      <td>7.370000</td>\n",
              "      <td>11.720000</td>\n",
              "      <td>11.020000</td>\n",
              "      <td>8.180000</td>\n",
              "      <td>7.330000</td>\n",
              "      <td>6.710000</td>\n",
              "      <td>8.520000</td>\n",
              "      <td>6.310000</td>\n",
              "      <td>7.18</td>\n",
              "      <td>8.070000</td>\n",
              "      <td>8.090000</td>\n",
              "      <td>9.530000</td>\n",
              "      <td>5.480000</td>\n",
              "      <td>8.750000</td>\n",
              "      <td>9.300000</td>\n",
              "      <td>7.540000</td>\n",
              "      <td>9.160000</td>\n",
              "      <td>6.740000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01D6177B5D4FFE0CABA9EF17DAFC2B84</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4B75AC4F2D8434CFF62DB64D0BB43103</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.34</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>12.73</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>10.95</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "      <td>13.483126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B32AC8CC6D5D805AC053557AB05F5343</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>17.30</td>\n",
              "      <td>16.95</td>\n",
              "      <td>8.25</td>\n",
              "      <td>22.76</td>\n",
              "      <td>14.07</td>\n",
              "      <td>20.70</td>\n",
              "      <td>6.50</td>\n",
              "      <td>9.99</td>\n",
              "      <td>18.59</td>\n",
              "      <td>16.79</td>\n",
              "      <td>26.82</td>\n",
              "      <td>14.70</td>\n",
              "      <td>16.05</td>\n",
              "      <td>24.10</td>\n",
              "      <td>12.64</td>\n",
              "      <td>10.85</td>\n",
              "      <td>11.770000</td>\n",
              "      <td>13.030000</td>\n",
              "      <td>12.260000</td>\n",
              "      <td>15.660000</td>\n",
              "      <td>13.43</td>\n",
              "      <td>15.030000</td>\n",
              "      <td>14.610000</td>\n",
              "      <td>13.970000</td>\n",
              "      <td>15.070000</td>\n",
              "      <td>14.110000</td>\n",
              "      <td>14.770000</td>\n",
              "      <td>11.720000</td>\n",
              "      <td>11.730000</td>\n",
              "      <td>11.980000</td>\n",
              "      <td>12.81</td>\n",
              "      <td>15.120000</td>\n",
              "      <td>17.260000</td>\n",
              "      <td>14.910000</td>\n",
              "      <td>19.590000</td>\n",
              "      <td>20.790000</td>\n",
              "      <td>17.950000</td>\n",
              "      <td>19.260000</td>\n",
              "      <td>14.460000</td>\n",
              "      <td>11.720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EDFC78B07BA2908B3395C4EB2304665E</td>\n",
              "      <td>1</td>\n",
              "      <td>2.9</td>\n",
              "      <td>5.64</td>\n",
              "      <td>6.99</td>\n",
              "      <td>3.32</td>\n",
              "      <td>3.61</td>\n",
              "      <td>5.35</td>\n",
              "      <td>4.73</td>\n",
              "      <td>3.68</td>\n",
              "      <td>3.53</td>\n",
              "      <td>3.42</td>\n",
              "      <td>3.81</td>\n",
              "      <td>4.58</td>\n",
              "      <td>3.56</td>\n",
              "      <td>4.25</td>\n",
              "      <td>3.86</td>\n",
              "      <td>3.53</td>\n",
              "      <td>3.41</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.88</td>\n",
              "      <td>3.81</td>\n",
              "      <td>6.51</td>\n",
              "      <td>9.19</td>\n",
              "      <td>7.79</td>\n",
              "      <td>7.02</td>\n",
              "      <td>7.25</td>\n",
              "      <td>6.37</td>\n",
              "      <td>6.43</td>\n",
              "      <td>7.89</td>\n",
              "      <td>7.46</td>\n",
              "      <td>7.95</td>\n",
              "      <td>7.59</td>\n",
              "      <td>4.84</td>\n",
              "      <td>6.06</td>\n",
              "      <td>5.6</td>\n",
              "      <td>6.72</td>\n",
              "      <td>7.29</td>\n",
              "      <td>5.6</td>\n",
              "      <td>6.28</td>\n",
              "      <td>...</td>\n",
              "      <td>9.56</td>\n",
              "      <td>9.48</td>\n",
              "      <td>10.60</td>\n",
              "      <td>10.06</td>\n",
              "      <td>10.79</td>\n",
              "      <td>10.91</td>\n",
              "      <td>17.77</td>\n",
              "      <td>10.37</td>\n",
              "      <td>13.51</td>\n",
              "      <td>14.13</td>\n",
              "      <td>17.44</td>\n",
              "      <td>15.96</td>\n",
              "      <td>12.18</td>\n",
              "      <td>18.54</td>\n",
              "      <td>13.44</td>\n",
              "      <td>11.68</td>\n",
              "      <td>9.150000</td>\n",
              "      <td>9.160000</td>\n",
              "      <td>10.190000</td>\n",
              "      <td>10.310000</td>\n",
              "      <td>8.03</td>\n",
              "      <td>10.080000</td>\n",
              "      <td>10.340000</td>\n",
              "      <td>11.240000</td>\n",
              "      <td>15.140000</td>\n",
              "      <td>14.360000</td>\n",
              "      <td>12.390000</td>\n",
              "      <td>10.360000</td>\n",
              "      <td>9.290000</td>\n",
              "      <td>7.910000</td>\n",
              "      <td>14.21</td>\n",
              "      <td>10.220000</td>\n",
              "      <td>8.470000</td>\n",
              "      <td>6.110000</td>\n",
              "      <td>6.100000</td>\n",
              "      <td>6.730000</td>\n",
              "      <td>7.520000</td>\n",
              "      <td>10.890000</td>\n",
              "      <td>9.860000</td>\n",
              "      <td>8.720000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 1036 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            CONS_NO  FLAG  ...  2016-10-30  2016-10-31\n",
              "0  0387DD8A07E07FDA6271170F86AD9151     1  ...    9.160000    6.740000\n",
              "1  01D6177B5D4FFE0CABA9EF17DAFC2B84     1  ...    0.000000    0.000000\n",
              "2  4B75AC4F2D8434CFF62DB64D0BB43103     1  ...   13.483126   13.483126\n",
              "3  B32AC8CC6D5D805AC053557AB05F5343     1  ...   14.460000   11.720000\n",
              "4  EDFC78B07BA2908B3395C4EB2304665E     1  ...    9.860000    8.720000\n",
              "\n",
              "[5 rows x 1036 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77MdpldMMllJ",
        "colab_type": "text"
      },
      "source": [
        "# Extracting Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRJZUOL8M5W2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Column 2 onwards are kWhs, column 1 is the FLAG\n",
        "kWhs = raw_data.iloc[:, 2:]\n",
        "labels = raw_data.iloc[:, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg-h4Xe3M9DQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting number of rows and columns in the feature matrix\n",
        "NUM_CONSUMERS, NUM_DAYS = kWhs.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XBM4S9INLAw",
        "colab_type": "code",
        "outputId": "a1938a41-d3bc-4d7e-9fb9-26de08a228ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(NUM_CONSUMERS, NUM_DAYS)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42372 1034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzi-zonnNVYc",
        "colab_type": "text"
      },
      "source": [
        "# Scaling\n",
        "\n",
        "Defining a single function to help scale data according to specified scaling strategy.\n",
        "\n",
        "Under the hood it uses a `Scikit-Learn` feature scaler object to apply the right kind of feature scaling to the data. \n",
        "\n",
        "All you need to do is specify one of following values for the `scaling_strategy` argument\n",
        "- `Standard`: all features will have mean of 0, standard deviation of 1.\n",
        "- `MinMax`: all feature values `x` will be scaled according to `(x - x_min)/(x_max - x_min)`, where `x_min` and `x_max` are the minimum and maximum values for that feature.\n",
        "- `MaxAbs`: all feature valeus `x` will be scaled according t `(x / abs(x_max))` where `x_max` is the maximum value of the feature, and `abs` is a function that finds its absolute value. \n",
        "\n",
        "Zheng has used `MinMax`, but my results show `Standard` to be more effective. \n",
        "\n",
        "You can also specify whether or not you want to compare the distribution of kWhs before and after scaling using the `plot_distributions` argument. **Plotting distributions will cause the function to take a little more time than usual to execute**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGI-GgoNX70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scale_features(X=kWhs, scaling_strategy='Standard', plot_distributions=False):\n",
        "  \"\"\"Allows user to scale features for individual samples using specified scaling strategy.\n",
        "  `Standard` - all features scaled to have mean 0, standard deviation of 1\n",
        "  `MinMax` - all features scaled according to x - x_min / (x_max - x_min)\n",
        "  `MaxAbs` - all features scaled according to x / abs(x_max)\n",
        "  \n",
        "  Returns matrix of scaled features as a numpy array.\n",
        "\n",
        "  Also allows before/after plots of feature values to compare effect of scaling\n",
        "  \"\"\"\n",
        "  # StandardScaler - all features will have mean of 0, and std deviation of 1\n",
        "  if (scaling_strategy=='Standard'):\n",
        "    # Instantiate a StandardScaler object\n",
        "    scaler = StandardScaler()\n",
        "  elif (scaling_strategy=='MinMax'):\n",
        "    scaler = MinMaxScaler()\n",
        "  elif (scaling_strategy=='MaxAbs'):\n",
        "    scaler = MaxAbsScaler()\n",
        "\n",
        "  # Make a copy of the features, transpose so that consumers become columns\n",
        "  X_copy = X.copy().transpose()\n",
        "\n",
        "  # Once consumers are in columns, scale each consumer using scaler object and retranspose results \n",
        "  # Retransposing ensures that in scaled data, the consumers are still rows\n",
        "  X_scaled = scaler.fit_transform(X_copy).transpose() # THIS IS A NUMPY ARRAY, NOT A DATAFRAME\n",
        "\n",
        "  # If the user has asked to compare distributions\n",
        "  if plot_distributions:\n",
        "    # First figure - before scaling\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    distplot(X.values.ravel(), kde=False, hist_kws=dict(edgecolor='k', linewidth=2))\n",
        "    plt.xlabel('Unscaled kWh Values', fontsize=14)\n",
        "    plt.ylabel('Frequency', fontsize=14)\n",
        "    plt.title('kWh Values - Before Scaling', fontsize=18)\n",
        "\n",
        "    # Second figure - after scaling\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    distplot(X_scaled.ravel(), kde=False, hist_kws=dict(edgecolor='k', linewidth=2))\n",
        "    plt.xlabel('Scaled kWh Values', fontsize=14)\n",
        "    plt.ylabel('Frequency', fontsize=14)\n",
        "    plt.title('kWh Values - After {} Scaling'.format(scaling_strategy), fontsize=18)\n",
        "\n",
        "  # Return the scaled features\n",
        "  return X_scaled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU50Kl-nQZ2S",
        "colab_type": "code",
        "outputId": "ad3ecc90-b15b-4fdf-a014-908224f40bf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "source": [
        "X_scaled_std = scale_features(X=kWhs, plot_distributions=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAAGLCAYAAAAxuN8qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhGUAAADABGEZAAAAMEFYBgAAAEz8f3VmNkwFakmSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAGLCAYAAADnHQVPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYAK_99wVjqL",
        "colab_type": "text"
      },
      "source": [
        "# Adding 2 Columns of Zeroes to Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "831ydBCQVkwR",
        "colab_type": "code",
        "outputId": "6e9535e9-80a6-4a53-be74-65f06b1535ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "NUM_DAYS = 1034\n",
        "DAYS_PER_WEEK = 7\n",
        "NUM_WEEKS_FLOAT = NUM_DAYS / DAYS_PER_WEEK \n",
        "NUM_WEEKS = int(np.ceil(NUM_WEEKS_FLOAT))\n",
        "print(f\"Number of days per dataset: {NUM_DAYS}.\\nNumber of days per week: {DAYS_PER_WEEK}\")\n",
        "print(f\"So exact number of weeks: {NUM_WEEKS_FLOAT}\\nWhich, when rounded up is {NUM_WEEKS}\")\n",
        "print(f\"Which means new number of days is: {NUM_WEEKS * DAYS_PER_WEEK}\")\n",
        "print(f\"Extra days to add: {NUM_WEEKS * DAYS_PER_WEEK - NUM_DAYS}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of days per dataset: 1034.\n",
            "Number of days per week: 7\n",
            "So exact number of weeks: 147.71428571428572\n",
            "Which, when rounded up is 148\n",
            "Which means new number of days is: 1036\n",
            "Extra days to add: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i5ikytuW8w0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 extra days per consumer \n",
        "X_scaled_std_padded = np.append(X_scaled_std, np.zeros((NUM_CONSUMERS, 2)), \n",
        "                                axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsEuJEMWXcoq",
        "colab_type": "code",
        "outputId": "d35841a1-5464-4f71-9a12-4fa045371682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Does it have the right shape?\n",
        "X_scaled_std_padded.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42372, 1036)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3ijwB1vXcke",
        "colab_type": "code",
        "outputId": "15cd4132-a823-4c6b-9bd2-656d8a32bcc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "# Are the last two columns (and only the last two columns) zero?\n",
        "pd.DataFrame(X_scaled_std_padded)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "      <th>1024</th>\n",
              "      <th>1025</th>\n",
              "      <th>1026</th>\n",
              "      <th>1027</th>\n",
              "      <th>1028</th>\n",
              "      <th>1029</th>\n",
              "      <th>1030</th>\n",
              "      <th>1031</th>\n",
              "      <th>1032</th>\n",
              "      <th>1033</th>\n",
              "      <th>1034</th>\n",
              "      <th>1035</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063220</td>\n",
              "      <td>0.561141</td>\n",
              "      <td>0.028722</td>\n",
              "      <td>0.106917</td>\n",
              "      <td>0.284007</td>\n",
              "      <td>0.265608</td>\n",
              "      <td>-0.005776</td>\n",
              "      <td>1.212002</td>\n",
              "      <td>0.377151</td>\n",
              "      <td>0.432348</td>\n",
              "      <td>1.410940</td>\n",
              "      <td>0.350703</td>\n",
              "      <td>0.178213</td>\n",
              "      <td>-0.101220</td>\n",
              "      <td>-0.102370</td>\n",
              "      <td>-0.172516</td>\n",
              "      <td>-0.132269</td>\n",
              "      <td>0.049421</td>\n",
              "      <td>0.036771</td>\n",
              "      <td>0.133366</td>\n",
              "      <td>-0.032224</td>\n",
              "      <td>0.467996</td>\n",
              "      <td>0.387501</td>\n",
              "      <td>0.060920</td>\n",
              "      <td>-0.036824</td>\n",
              "      <td>-0.108120</td>\n",
              "      <td>0.100018</td>\n",
              "      <td>-0.154117</td>\n",
              "      <td>-0.054073</td>\n",
              "      <td>0.048271</td>\n",
              "      <td>0.050571</td>\n",
              "      <td>0.216161</td>\n",
              "      <td>-0.249562</td>\n",
              "      <td>0.126466</td>\n",
              "      <td>0.189712</td>\n",
              "      <td>-0.012676</td>\n",
              "      <td>0.173613</td>\n",
              "      <td>-0.104670</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>1.440577</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.463751</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.048888</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>...</td>\n",
              "      <td>0.207467</td>\n",
              "      <td>1.742458</td>\n",
              "      <td>0.823156</td>\n",
              "      <td>1.524534</td>\n",
              "      <td>0.022337</td>\n",
              "      <td>0.391539</td>\n",
              "      <td>1.301320</td>\n",
              "      <td>1.110901</td>\n",
              "      <td>2.171960</td>\n",
              "      <td>0.889803</td>\n",
              "      <td>1.032617</td>\n",
              "      <td>1.884215</td>\n",
              "      <td>0.671879</td>\n",
              "      <td>0.482517</td>\n",
              "      <td>0.579842</td>\n",
              "      <td>0.713136</td>\n",
              "      <td>0.631679</td>\n",
              "      <td>0.991360</td>\n",
              "      <td>0.755451</td>\n",
              "      <td>0.924713</td>\n",
              "      <td>0.880282</td>\n",
              "      <td>0.812577</td>\n",
              "      <td>0.928945</td>\n",
              "      <td>0.827388</td>\n",
              "      <td>0.897208</td>\n",
              "      <td>0.574553</td>\n",
              "      <td>0.575611</td>\n",
              "      <td>0.602058</td>\n",
              "      <td>0.689863</td>\n",
              "      <td>0.934234</td>\n",
              "      <td>1.160621</td>\n",
              "      <td>0.912018</td>\n",
              "      <td>1.407109</td>\n",
              "      <td>1.534055</td>\n",
              "      <td>1.233616</td>\n",
              "      <td>1.372199</td>\n",
              "      <td>0.864414</td>\n",
              "      <td>0.574553</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.095170</td>\n",
              "      <td>-0.485963</td>\n",
              "      <td>-0.185806</td>\n",
              "      <td>-1.001788</td>\n",
              "      <td>-0.937310</td>\n",
              "      <td>-0.550441</td>\n",
              "      <td>-0.688291</td>\n",
              "      <td>-0.921746</td>\n",
              "      <td>-0.955097</td>\n",
              "      <td>-0.979554</td>\n",
              "      <td>-0.892842</td>\n",
              "      <td>-0.721642</td>\n",
              "      <td>-0.948427</td>\n",
              "      <td>-0.795013</td>\n",
              "      <td>-0.881725</td>\n",
              "      <td>-0.955097</td>\n",
              "      <td>-0.981778</td>\n",
              "      <td>-1.550963</td>\n",
              "      <td>-1.544293</td>\n",
              "      <td>-0.892842</td>\n",
              "      <td>-0.292529</td>\n",
              "      <td>0.303338</td>\n",
              "      <td>-0.007936</td>\n",
              "      <td>-0.179136</td>\n",
              "      <td>-0.127999</td>\n",
              "      <td>-0.323656</td>\n",
              "      <td>-0.310316</td>\n",
              "      <td>0.014298</td>\n",
              "      <td>-0.081308</td>\n",
              "      <td>0.027638</td>\n",
              "      <td>-0.052404</td>\n",
              "      <td>-0.663834</td>\n",
              "      <td>-0.392581</td>\n",
              "      <td>-0.494857</td>\n",
              "      <td>-0.245838</td>\n",
              "      <td>-0.119105</td>\n",
              "      <td>-0.494857</td>\n",
              "      <td>-0.343667</td>\n",
              "      <td>-0.243614</td>\n",
              "      <td>-0.190253</td>\n",
              "      <td>...</td>\n",
              "      <td>0.616835</td>\n",
              "      <td>0.496772</td>\n",
              "      <td>0.659079</td>\n",
              "      <td>0.685760</td>\n",
              "      <td>2.211000</td>\n",
              "      <td>0.565697</td>\n",
              "      <td>1.263839</td>\n",
              "      <td>1.401689</td>\n",
              "      <td>2.137628</td>\n",
              "      <td>1.808568</td>\n",
              "      <td>0.968129</td>\n",
              "      <td>2.382201</td>\n",
              "      <td>1.248275</td>\n",
              "      <td>0.856960</td>\n",
              "      <td>0.294444</td>\n",
              "      <td>0.296668</td>\n",
              "      <td>0.525676</td>\n",
              "      <td>0.552357</td>\n",
              "      <td>0.045425</td>\n",
              "      <td>0.501219</td>\n",
              "      <td>0.559027</td>\n",
              "      <td>0.759131</td>\n",
              "      <td>1.626250</td>\n",
              "      <td>1.452827</td>\n",
              "      <td>1.014820</td>\n",
              "      <td>0.563473</td>\n",
              "      <td>0.325572</td>\n",
              "      <td>0.018745</td>\n",
              "      <td>1.419476</td>\n",
              "      <td>0.532346</td>\n",
              "      <td>0.143254</td>\n",
              "      <td>-0.381464</td>\n",
              "      <td>-0.383688</td>\n",
              "      <td>-0.243614</td>\n",
              "      <td>-0.067967</td>\n",
              "      <td>0.681313</td>\n",
              "      <td>0.452304</td>\n",
              "      <td>0.198839</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42367</th>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>-1.473365</td>\n",
              "      <td>...</td>\n",
              "      <td>0.062085</td>\n",
              "      <td>0.309591</td>\n",
              "      <td>-0.157920</td>\n",
              "      <td>0.492928</td>\n",
              "      <td>0.474594</td>\n",
              "      <td>0.158337</td>\n",
              "      <td>0.538762</td>\n",
              "      <td>0.492928</td>\n",
              "      <td>0.318757</td>\n",
              "      <td>0.295840</td>\n",
              "      <td>0.085002</td>\n",
              "      <td>0.749600</td>\n",
              "      <td>0.456260</td>\n",
              "      <td>0.295840</td>\n",
              "      <td>0.130837</td>\n",
              "      <td>0.144587</td>\n",
              "      <td>-0.148753</td>\n",
              "      <td>0.103336</td>\n",
              "      <td>-0.153336</td>\n",
              "      <td>-0.107502</td>\n",
              "      <td>-0.034167</td>\n",
              "      <td>-0.020417</td>\n",
              "      <td>0.089586</td>\n",
              "      <td>-0.098335</td>\n",
              "      <td>0.030001</td>\n",
              "      <td>-0.121252</td>\n",
              "      <td>-0.157920</td>\n",
              "      <td>-0.043334</td>\n",
              "      <td>0.025418</td>\n",
              "      <td>-0.052501</td>\n",
              "      <td>-0.212921</td>\n",
              "      <td>-0.093752</td>\n",
              "      <td>-0.102918</td>\n",
              "      <td>-0.176253</td>\n",
              "      <td>-0.309173</td>\n",
              "      <td>0.085002</td>\n",
              "      <td>0.172088</td>\n",
              "      <td>-0.309173</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42368</th>\n",
              "      <td>-1.027324</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>0.615467</td>\n",
              "      <td>0.794978</td>\n",
              "      <td>0.664425</td>\n",
              "      <td>-0.826055</td>\n",
              "      <td>-0.298404</td>\n",
              "      <td>0.593708</td>\n",
              "      <td>-0.108014</td>\n",
              "      <td>-0.347361</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>1.757806</td>\n",
              "      <td>2.075916</td>\n",
              "      <td>-0.189609</td>\n",
              "      <td>-0.189609</td>\n",
              "      <td>-0.711821</td>\n",
              "      <td>0.093255</td>\n",
              "      <td>0.528432</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>-2.496045</td>\n",
              "      <td>1.034325</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064496</td>\n",
              "      <td>-0.390879</td>\n",
              "      <td>-0.156971</td>\n",
              "      <td>0.338042</td>\n",
              "      <td>0.120454</td>\n",
              "      <td>0.153092</td>\n",
              "      <td>-0.499673</td>\n",
              "      <td>-0.260326</td>\n",
              "      <td>-0.162411</td>\n",
              "      <td>0.403319</td>\n",
              "      <td>-0.989246</td>\n",
              "      <td>-0.287524</td>\n",
              "      <td>0.267326</td>\n",
              "      <td>0.463155</td>\n",
              "      <td>-0.146092</td>\n",
              "      <td>0.256447</td>\n",
              "      <td>-0.178730</td>\n",
              "      <td>-0.706382</td>\n",
              "      <td>-0.673743</td>\n",
              "      <td>-0.118893</td>\n",
              "      <td>0.376120</td>\n",
              "      <td>0.093255</td>\n",
              "      <td>0.202050</td>\n",
              "      <td>-0.717261</td>\n",
              "      <td>-0.369120</td>\n",
              "      <td>-0.064496</td>\n",
              "      <td>0.289085</td>\n",
              "      <td>-0.521432</td>\n",
              "      <td>-0.407198</td>\n",
              "      <td>1.105041</td>\n",
              "      <td>-0.798857</td>\n",
              "      <td>0.310844</td>\n",
              "      <td>-0.526871</td>\n",
              "      <td>0.027979</td>\n",
              "      <td>-0.477914</td>\n",
              "      <td>0.887453</td>\n",
              "      <td>0.794978</td>\n",
              "      <td>0.098695</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42369</th>\n",
              "      <td>-0.454088</td>\n",
              "      <td>-0.007993</td>\n",
              "      <td>-0.192584</td>\n",
              "      <td>-0.146436</td>\n",
              "      <td>0.284277</td>\n",
              "      <td>0.161216</td>\n",
              "      <td>-0.215657</td>\n",
              "      <td>-0.361792</td>\n",
              "      <td>0.045847</td>\n",
              "      <td>-0.254114</td>\n",
              "      <td>0.338116</td>\n",
              "      <td>-0.207966</td>\n",
              "      <td>-0.438705</td>\n",
              "      <td>-0.269496</td>\n",
              "      <td>-0.215657</td>\n",
              "      <td>-0.877109</td>\n",
              "      <td>-0.338718</td>\n",
              "      <td>0.391955</td>\n",
              "      <td>0.076612</td>\n",
              "      <td>-0.115671</td>\n",
              "      <td>-0.269496</td>\n",
              "      <td>-0.084905</td>\n",
              "      <td>-0.407940</td>\n",
              "      <td>-0.269496</td>\n",
              "      <td>-0.454088</td>\n",
              "      <td>0.515015</td>\n",
              "      <td>-0.431014</td>\n",
              "      <td>-0.423322</td>\n",
              "      <td>0.653459</td>\n",
              "      <td>-0.038758</td>\n",
              "      <td>0.561163</td>\n",
              "      <td>1.437971</td>\n",
              "      <td>-0.154127</td>\n",
              "      <td>-0.200275</td>\n",
              "      <td>0.007390</td>\n",
              "      <td>0.245820</td>\n",
              "      <td>-0.015684</td>\n",
              "      <td>-0.115671</td>\n",
              "      <td>-0.184892</td>\n",
              "      <td>0.299659</td>\n",
              "      <td>...</td>\n",
              "      <td>0.661150</td>\n",
              "      <td>0.830358</td>\n",
              "      <td>0.130451</td>\n",
              "      <td>-0.900182</td>\n",
              "      <td>-0.646370</td>\n",
              "      <td>-0.392557</td>\n",
              "      <td>0.914963</td>\n",
              "      <td>0.761137</td>\n",
              "      <td>2.322469</td>\n",
              "      <td>1.153393</td>\n",
              "      <td>1.784079</td>\n",
              "      <td>2.637812</td>\n",
              "      <td>0.684224</td>\n",
              "      <td>0.422720</td>\n",
              "      <td>0.191981</td>\n",
              "      <td>-0.515618</td>\n",
              "      <td>-0.469470</td>\n",
              "      <td>-0.415631</td>\n",
              "      <td>-0.507927</td>\n",
              "      <td>-0.461779</td>\n",
              "      <td>-0.346409</td>\n",
              "      <td>-0.400248</td>\n",
              "      <td>-0.238731</td>\n",
              "      <td>-0.361792</td>\n",
              "      <td>0.122759</td>\n",
              "      <td>-0.261805</td>\n",
              "      <td>-0.369483</td>\n",
              "      <td>-0.346409</td>\n",
              "      <td>-0.138745</td>\n",
              "      <td>-0.431014</td>\n",
              "      <td>-0.400248</td>\n",
              "      <td>-0.477161</td>\n",
              "      <td>-0.523309</td>\n",
              "      <td>-0.507927</td>\n",
              "      <td>-0.292570</td>\n",
              "      <td>-0.392557</td>\n",
              "      <td>-0.600222</td>\n",
              "      <td>-0.400248</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42370</th>\n",
              "      <td>0.248615</td>\n",
              "      <td>0.030794</td>\n",
              "      <td>0.547806</td>\n",
              "      <td>0.286170</td>\n",
              "      <td>0.237348</td>\n",
              "      <td>0.322474</td>\n",
              "      <td>0.370044</td>\n",
              "      <td>1.137425</td>\n",
              "      <td>-0.320975</td>\n",
              "      <td>-0.133198</td>\n",
              "      <td>-0.175761</td>\n",
              "      <td>-0.038058</td>\n",
              "      <td>-0.055584</td>\n",
              "      <td>-0.320975</td>\n",
              "      <td>0.058334</td>\n",
              "      <td>0.263637</td>\n",
              "      <td>-0.180768</td>\n",
              "      <td>0.582858</td>\n",
              "      <td>-0.299693</td>\n",
              "      <td>-0.006762</td>\n",
              "      <td>-0.397337</td>\n",
              "      <td>0.124682</td>\n",
              "      <td>-0.197042</td>\n",
              "      <td>-0.008013</td>\n",
              "      <td>0.114667</td>\n",
              "      <td>0.204800</td>\n",
              "      <td>-1.028267</td>\n",
              "      <td>-1.208533</td>\n",
              "      <td>0.125934</td>\n",
              "      <td>1.256350</td>\n",
              "      <td>0.113416</td>\n",
              "      <td>0.062090</td>\n",
              "      <td>-0.356026</td>\n",
              "      <td>-0.110665</td>\n",
              "      <td>-0.302197</td>\n",
              "      <td>-0.750358</td>\n",
              "      <td>-1.560302</td>\n",
              "      <td>-1.587842</td>\n",
              "      <td>-0.699032</td>\n",
              "      <td>-0.076865</td>\n",
              "      <td>...</td>\n",
              "      <td>0.649205</td>\n",
              "      <td>0.159734</td>\n",
              "      <td>-0.061843</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.779397</td>\n",
              "      <td>-0.144465</td>\n",
              "      <td>0.491473</td>\n",
              "      <td>-0.016776</td>\n",
              "      <td>-0.339752</td>\n",
              "      <td>0.989707</td>\n",
              "      <td>1.193758</td>\n",
              "      <td>0.174756</td>\n",
              "      <td>-0.207057</td>\n",
              "      <td>-0.150724</td>\n",
              "      <td>-0.144465</td>\n",
              "      <td>-0.454922</td>\n",
              "      <td>-0.105657</td>\n",
              "      <td>0.329985</td>\n",
              "      <td>-0.096894</td>\n",
              "      <td>-0.364789</td>\n",
              "      <td>0.029542</td>\n",
              "      <td>-0.260886</td>\n",
              "      <td>0.201045</td>\n",
              "      <td>-0.573848</td>\n",
              "      <td>-0.200798</td>\n",
              "      <td>-0.215820</td>\n",
              "      <td>-0.338501</td>\n",
              "      <td>-0.515011</td>\n",
              "      <td>0.092134</td>\n",
              "      <td>0.197289</td>\n",
              "      <td>-0.233346</td>\n",
              "      <td>-0.565085</td>\n",
              "      <td>-0.363538</td>\n",
              "      <td>-0.469944</td>\n",
              "      <td>-0.335997</td>\n",
              "      <td>-0.218323</td>\n",
              "      <td>-0.197042</td>\n",
              "      <td>-0.565085</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42371</th>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>...</td>\n",
              "      <td>1.083274</td>\n",
              "      <td>1.936714</td>\n",
              "      <td>1.776694</td>\n",
              "      <td>0.826273</td>\n",
              "      <td>0.084362</td>\n",
              "      <td>0.484412</td>\n",
              "      <td>0.644432</td>\n",
              "      <td>1.539089</td>\n",
              "      <td>2.355150</td>\n",
              "      <td>1.606976</td>\n",
              "      <td>2.355150</td>\n",
              "      <td>2.355150</td>\n",
              "      <td>1.929440</td>\n",
              "      <td>2.295547</td>\n",
              "      <td>1.606976</td>\n",
              "      <td>1.153586</td>\n",
              "      <td>0.668677</td>\n",
              "      <td>0.377732</td>\n",
              "      <td>0.811725</td>\n",
              "      <td>0.705045</td>\n",
              "      <td>0.675951</td>\n",
              "      <td>1.408163</td>\n",
              "      <td>1.871251</td>\n",
              "      <td>2.355150</td>\n",
              "      <td>1.684561</td>\n",
              "      <td>2.157348</td>\n",
              "      <td>2.251905</td>\n",
              "      <td>2.355150</td>\n",
              "      <td>1.965808</td>\n",
              "      <td>2.355150</td>\n",
              "      <td>1.429984</td>\n",
              "      <td>1.509994</td>\n",
              "      <td>0.954773</td>\n",
              "      <td>1.335427</td>\n",
              "      <td>1.376644</td>\n",
              "      <td>2.193716</td>\n",
              "      <td>1.141463</td>\n",
              "      <td>0.678375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>42372 rows Ã— 1036 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2         3     ...      1032      1033  1034  1035\n",
              "0     -0.879724 -0.879724 -0.879724 -0.879724  ...  0.173613 -0.104670   0.0   0.0\n",
              "1      0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0   0.0\n",
              "2     -0.503221 -0.503221 -0.503221 -0.503221  ...  2.639282  2.639282   0.0   0.0\n",
              "3     -0.665288 -0.665288 -0.665288 -0.665288  ...  0.864414  0.574553   0.0   0.0\n",
              "4     -1.095170 -0.485963 -0.185806 -1.001788  ...  0.452304  0.198839   0.0   0.0\n",
              "...         ...       ...       ...       ...  ...       ...       ...   ...   ...\n",
              "42367 -1.473365 -1.473365 -1.473365 -1.473365  ...  0.172088 -0.309173   0.0   0.0\n",
              "42368 -1.027324 -2.496045 -2.496045  0.615467  ...  0.794978  0.098695   0.0   0.0\n",
              "42369 -0.454088 -0.007993 -0.192584 -0.146436  ... -0.600222 -0.400248   0.0   0.0\n",
              "42370  0.248615  0.030794  0.547806  0.286170  ... -0.197042 -0.565085   0.0   0.0\n",
              "42371 -0.594511 -0.594511 -0.594511 -0.594511  ...  1.141463  0.678375   0.0   0.0\n",
              "\n",
              "[42372 rows x 1036 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SAow28gf2-7",
        "colab_type": "code",
        "outputId": "042b8480-cba5-45c9-c40e-c248899901d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "pd.DataFrame(X_scaled_std).head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "      <th>1024</th>\n",
              "      <th>1025</th>\n",
              "      <th>1026</th>\n",
              "      <th>1027</th>\n",
              "      <th>1028</th>\n",
              "      <th>1029</th>\n",
              "      <th>1030</th>\n",
              "      <th>1031</th>\n",
              "      <th>1032</th>\n",
              "      <th>1033</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>-0.879724</td>\n",
              "      <td>...</td>\n",
              "      <td>0.104617</td>\n",
              "      <td>0.325404</td>\n",
              "      <td>0.063220</td>\n",
              "      <td>0.561141</td>\n",
              "      <td>0.028722</td>\n",
              "      <td>0.106917</td>\n",
              "      <td>0.284007</td>\n",
              "      <td>0.265608</td>\n",
              "      <td>-0.005776</td>\n",
              "      <td>1.212002</td>\n",
              "      <td>0.377151</td>\n",
              "      <td>0.432348</td>\n",
              "      <td>1.410940</td>\n",
              "      <td>0.350703</td>\n",
              "      <td>0.178213</td>\n",
              "      <td>-0.101220</td>\n",
              "      <td>-0.102370</td>\n",
              "      <td>-0.172516</td>\n",
              "      <td>-0.132269</td>\n",
              "      <td>0.049421</td>\n",
              "      <td>0.036771</td>\n",
              "      <td>0.133366</td>\n",
              "      <td>-0.032224</td>\n",
              "      <td>0.467996</td>\n",
              "      <td>0.387501</td>\n",
              "      <td>0.060920</td>\n",
              "      <td>-0.036824</td>\n",
              "      <td>-0.108120</td>\n",
              "      <td>0.100018</td>\n",
              "      <td>-0.154117</td>\n",
              "      <td>-0.054073</td>\n",
              "      <td>0.048271</td>\n",
              "      <td>0.050571</td>\n",
              "      <td>0.216161</td>\n",
              "      <td>-0.249562</td>\n",
              "      <td>0.126466</td>\n",
              "      <td>0.189712</td>\n",
              "      <td>-0.012676</td>\n",
              "      <td>0.173613</td>\n",
              "      <td>-0.104670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>-0.503221</td>\n",
              "      <td>1.440577</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.463751</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.048888</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "      <td>2.639282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>-0.665288</td>\n",
              "      <td>...</td>\n",
              "      <td>1.164853</td>\n",
              "      <td>1.127827</td>\n",
              "      <td>0.207467</td>\n",
              "      <td>1.742458</td>\n",
              "      <td>0.823156</td>\n",
              "      <td>1.524534</td>\n",
              "      <td>0.022337</td>\n",
              "      <td>0.391539</td>\n",
              "      <td>1.301320</td>\n",
              "      <td>1.110901</td>\n",
              "      <td>2.171960</td>\n",
              "      <td>0.889803</td>\n",
              "      <td>1.032617</td>\n",
              "      <td>1.884215</td>\n",
              "      <td>0.671879</td>\n",
              "      <td>0.482517</td>\n",
              "      <td>0.579842</td>\n",
              "      <td>0.713136</td>\n",
              "      <td>0.631679</td>\n",
              "      <td>0.991360</td>\n",
              "      <td>0.755451</td>\n",
              "      <td>0.924713</td>\n",
              "      <td>0.880282</td>\n",
              "      <td>0.812577</td>\n",
              "      <td>0.928945</td>\n",
              "      <td>0.827388</td>\n",
              "      <td>0.897208</td>\n",
              "      <td>0.574553</td>\n",
              "      <td>0.575611</td>\n",
              "      <td>0.602058</td>\n",
              "      <td>0.689863</td>\n",
              "      <td>0.934234</td>\n",
              "      <td>1.160621</td>\n",
              "      <td>0.912018</td>\n",
              "      <td>1.407109</td>\n",
              "      <td>1.534055</td>\n",
              "      <td>1.233616</td>\n",
              "      <td>1.372199</td>\n",
              "      <td>0.864414</td>\n",
              "      <td>0.574553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.095170</td>\n",
              "      <td>-0.485963</td>\n",
              "      <td>-0.185806</td>\n",
              "      <td>-1.001788</td>\n",
              "      <td>-0.937310</td>\n",
              "      <td>-0.550441</td>\n",
              "      <td>-0.688291</td>\n",
              "      <td>-0.921746</td>\n",
              "      <td>-0.955097</td>\n",
              "      <td>-0.979554</td>\n",
              "      <td>-0.892842</td>\n",
              "      <td>-0.721642</td>\n",
              "      <td>-0.948427</td>\n",
              "      <td>-0.795013</td>\n",
              "      <td>-0.881725</td>\n",
              "      <td>-0.955097</td>\n",
              "      <td>-0.981778</td>\n",
              "      <td>-1.550963</td>\n",
              "      <td>-1.544293</td>\n",
              "      <td>-0.892842</td>\n",
              "      <td>-0.292529</td>\n",
              "      <td>0.303338</td>\n",
              "      <td>-0.007936</td>\n",
              "      <td>-0.179136</td>\n",
              "      <td>-0.127999</td>\n",
              "      <td>-0.323656</td>\n",
              "      <td>-0.310316</td>\n",
              "      <td>0.014298</td>\n",
              "      <td>-0.081308</td>\n",
              "      <td>0.027638</td>\n",
              "      <td>-0.052404</td>\n",
              "      <td>-0.663834</td>\n",
              "      <td>-0.392581</td>\n",
              "      <td>-0.494857</td>\n",
              "      <td>-0.245838</td>\n",
              "      <td>-0.119105</td>\n",
              "      <td>-0.494857</td>\n",
              "      <td>-0.343667</td>\n",
              "      <td>-0.243614</td>\n",
              "      <td>-0.190253</td>\n",
              "      <td>...</td>\n",
              "      <td>0.385603</td>\n",
              "      <td>0.367816</td>\n",
              "      <td>0.616835</td>\n",
              "      <td>0.496772</td>\n",
              "      <td>0.659079</td>\n",
              "      <td>0.685760</td>\n",
              "      <td>2.211000</td>\n",
              "      <td>0.565697</td>\n",
              "      <td>1.263839</td>\n",
              "      <td>1.401689</td>\n",
              "      <td>2.137628</td>\n",
              "      <td>1.808568</td>\n",
              "      <td>0.968129</td>\n",
              "      <td>2.382201</td>\n",
              "      <td>1.248275</td>\n",
              "      <td>0.856960</td>\n",
              "      <td>0.294444</td>\n",
              "      <td>0.296668</td>\n",
              "      <td>0.525676</td>\n",
              "      <td>0.552357</td>\n",
              "      <td>0.045425</td>\n",
              "      <td>0.501219</td>\n",
              "      <td>0.559027</td>\n",
              "      <td>0.759131</td>\n",
              "      <td>1.626250</td>\n",
              "      <td>1.452827</td>\n",
              "      <td>1.014820</td>\n",
              "      <td>0.563473</td>\n",
              "      <td>0.325572</td>\n",
              "      <td>0.018745</td>\n",
              "      <td>1.419476</td>\n",
              "      <td>0.532346</td>\n",
              "      <td>0.143254</td>\n",
              "      <td>-0.381464</td>\n",
              "      <td>-0.383688</td>\n",
              "      <td>-0.243614</td>\n",
              "      <td>-0.067967</td>\n",
              "      <td>0.681313</td>\n",
              "      <td>0.452304</td>\n",
              "      <td>0.198839</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 1034 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0         1         2     ...      1031      1032      1033\n",
              "0 -0.879724 -0.879724 -0.879724  ... -0.012676  0.173613 -0.104670\n",
              "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "2 -0.503221 -0.503221 -0.503221  ...  2.639282  2.639282  2.639282\n",
              "3 -0.665288 -0.665288 -0.665288  ...  1.372199  0.864414  0.574553\n",
              "4 -1.095170 -0.485963 -0.185806  ...  0.681313  0.452304  0.198839\n",
              "\n",
              "[5 rows x 1034 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elqxTwBAYXjR",
        "colab_type": "text"
      },
      "source": [
        "# Reshaping the Data\n",
        "\n",
        "Data will now be a 4-dimensional numpy array. This is because a `Conv2D` layer in the network expects a 4-dimensional array as an input. The axes of this array are as follows\n",
        "\n",
        "- **Axis 0**: Samples axis - The consumer number in a particular batch. For our entire features matrix, this should range **from 0 to 42371**.\n",
        "- **Axis 1**: Week axis - Each week of kWh values for that consumer (0 to 147)\n",
        "- **Axis 2**: Day axis - Each day of kWh values for a specific week (0 to 6)\n",
        "- **Axis 3**: Channels axis - `Conv2D` is meant to process images, which usually have a channels axis (1 channel for grayscale, 3 channels for RGB, HSV, LAB colour spaces). In this case, our data does not have multiple channels of readings per consumer. So channel should be **1**.\n",
        "\n",
        "It may help to think of each consumer's reshaped data as a `(148, 7)` grid of squares, and the entire training data to be a **cube** of 42,372 such grids stacked together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POX4_FHyYiB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a new numpy array to store the training data - 4 dimensional because\n",
        "# CNN expects 4D data - (samples, rows, cols, channels) - in our case 1 channel\n",
        "NUM_CHANNELS = 1\n",
        "X_reshaped = np.zeros((NUM_CONSUMERS, NUM_WEEKS, DAYS_PER_WEEK, NUM_CHANNELS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdOwg7ZHcqdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_daily_to_weekly(daily_kWhs):\n",
        "  \"\"\"Will simply reshape the 1D daily kWhs of a single consumer into 2D weekly data\"\"\" \n",
        "  return daily_kWhs.reshape(NUM_WEEKS, DAYS_PER_WEEK, NUM_CHANNELS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7nionF_cz2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying this function to every consumer in the original dataset\n",
        "# And saving the (148, 7) kWhs matrix for that consumer at the ith index in 3D array\n",
        "for i in range(0, NUM_CONSUMERS):\n",
        "  X_reshaped[i] = transform_daily_to_weekly(X_scaled_std_padded[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKoZ7TtBfvfZ",
        "colab_type": "text"
      },
      "source": [
        "## Indexing Reshaped Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOWICmOqhLfa",
        "colab_type": "code",
        "outputId": "579faf38-dbf4-4e35-82fa-d704410208b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Entire reshaped data's shape\n",
        "X_reshaped.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42372, 148, 7, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxwyP7EMdgzp",
        "colab_type": "code",
        "outputId": "7057bb15-6c82-4f83-cebe-251f0e1b9fad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "# To access the kWhs of the 1st consumer\n",
        "X_reshaped[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-0.87972438],\n",
              "        [-0.87972438],\n",
              "        [-0.87972438],\n",
              "        ...,\n",
              "        [-0.87972438],\n",
              "        [-0.87972438],\n",
              "        [-0.87972438]],\n",
              "\n",
              "       [[-0.87972438],\n",
              "        [-0.87972438],\n",
              "        [-0.87972438],\n",
              "        ...,\n",
              "        [-0.87972438],\n",
              "        [-0.87972438],\n",
              "        [-0.87972438]],\n",
              "\n",
              "       [[-0.87972438],\n",
              "        [-0.87972438],\n",
              "        [-0.87972438],\n",
              "        ...,\n",
              "        [-0.87972438],\n",
              "        [-0.87972438],\n",
              "        [-0.87972438]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 0.13336578],\n",
              "        [-0.03222444],\n",
              "        [ 0.46799602],\n",
              "        ...,\n",
              "        [ 0.06092006],\n",
              "        [-0.03682416],\n",
              "        [-0.10811995]],\n",
              "\n",
              "       [[ 0.10001775],\n",
              "        [-0.15411724],\n",
              "        [-0.05407315],\n",
              "        ...,\n",
              "        [ 0.05057067],\n",
              "        [ 0.2161609 ],\n",
              "        [-0.2495616 ]],\n",
              "\n",
              "       [[ 0.12646619],\n",
              "        [ 0.18971246],\n",
              "        [-0.01267559],\n",
              "        ...,\n",
              "        [-0.10467016],\n",
              "        [ 0.        ],\n",
              "        [ 0.        ]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-C0lv1bg-6s",
        "colab_type": "code",
        "outputId": "60aae8ca-f0ac-4c52-d183-3026af1bca8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Shape of this tensor?\n",
        "print(\"Shape of the 1st consumer's tensor: \", X_reshaped[0].shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the 1st consumer's tensor:  (148, 7, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJelEtT7dxf_",
        "colab_type": "code",
        "outputId": "6586c8ef-83bf-4d9c-b180-1842bca14598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# To access the kWhs of the 1st week for this consumer\n",
        "X_reshaped[0][0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.87972438],\n",
              "       [-0.87972438],\n",
              "       [-0.87972438],\n",
              "       [-0.87972438],\n",
              "       [-0.87972438],\n",
              "       [-0.87972438],\n",
              "       [-0.87972438]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6VZbd5VhCoq",
        "colab_type": "code",
        "outputId": "de7fca8b-0252-4e9c-d663-abd0e01003ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Shape of this tensor?\n",
        "print(\"Shape of the 1st consumer's 1st week's tensor\", X_reshaped[0][0].shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the 1st consumer's 1st week's tensor (7, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhOSuBidd-nY",
        "colab_type": "code",
        "outputId": "cf96b96e-4467-44df-cdf7-a2f8f9826d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To access the kWhs of the 1st day of the 1st week\n",
        "X_reshaped[0][0][0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.87972438])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFXKwRALeFB7",
        "colab_type": "code",
        "outputId": "51c84410-ad33-421d-df94-7a865803d8e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Comparing with the first consumer's original values\n",
        "X_scaled_std_padded[0, :15]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.87972438, -0.87972438, -0.87972438, -0.87972438, -0.87972438,\n",
              "       -0.87972438, -0.87972438, -0.87972438, -0.87972438, -0.87972438,\n",
              "       -0.87972438, -0.87972438, -0.87972438, -0.87972438, -0.87972438])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um5HnkEAeJdM",
        "colab_type": "text"
      },
      "source": [
        "Seem correct. All values are the same because first 15 valeues for this consumer were probably all the same i.e. 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_xXtz3jey9o",
        "colab_type": "text"
      },
      "source": [
        "## Train-Test Split\n",
        "\n",
        "I am using a function from `sklearn` called `train_test_split` that will divide the features and labels into **stratified** training and test sets.\n",
        "\n",
        "This function returns four `numpy` arrays: `X_train`, `X_test`, `y_train`, and `y_test` (in that order) which are the features and labels for the training and test sets respectively. \n",
        "\n",
        "Stratified means both training and test sets will have the same proportion of samples belonging to classes 0 and 1. Since we will be stratifying on the basis of the labels, we need to specify this in the `stratify` argument for this function. \n",
        "\n",
        "By specifying `test_size` = 0.2, I am reserving 20% of the samples for the test set and 80% of samples for the training set.\n",
        "\n",
        "I am also going to specify a `random_state` so that the random number generator will generate the same indexes for train and test sets every time the notebook is run. This is useful to ensure that every time we run the notebook, the training and test sets contain the same samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCKCWmLvfEOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, \n",
        "                                                    labels, \n",
        "                                                    stratify=labels, \n",
        "                                                    test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLaP3cUafGm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confirming train-test split performed correctly\n",
        "assert len(X_train) == len(y_train)\n",
        "assert len(X_test) == len(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BMHYyF0eSls",
        "colab_type": "text"
      },
      "source": [
        "# Next Steps \n",
        "\n",
        "Now the CNN should work. All your data is 2-dimensional `(148, 7)`, and you have one label in `y` for each 3D tensor of a sample. \n",
        "\n",
        "The 3D tensor is, in effect, a 2D tensor with just one channel. This is a hack so that we can use a `Conv2D` layer with non-image data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1jWy7PN7jCc",
        "colab_type": "text"
      },
      "source": [
        "# CNN 1 - Conv2D/MaxPooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-Pbd4PNt1Va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebOiRFf0t7Nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggdrfeUDt8P4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First conv-pooling pair\n",
        "model.add(Conv2D(input_shape=(148, 7, 1), kernel_size=(3, 3), filters=18, \n",
        "                 activation='tanh', padding='same', data_format='channels_last', ))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHUfIPxwxdmc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "fee9aa32-98bd-48dc-e5bd-39bfe5a5e984"
      },
      "source": [
        "# After this pair\n",
        "model.summary()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_47 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 74, 3, 18)         0         \n",
            "=================================================================\n",
            "Total params: 180\n",
            "Trainable params: 180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVcYTdoqzE2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding second conv-pooling layer\n",
        "model.add(Conv2D(filters=36, kernel_size=(3, 3), activation='tanh', padding='same', \n",
        "                 data_format='channels_last'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE3VWik-zW0D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "54589a37-1d29-447c-8f5d-e4ce2eb9f23d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_47 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 74, 3, 18)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 74, 3, 36)         5868      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 37, 1, 36)         0         \n",
            "=================================================================\n",
            "Total params: 6,048\n",
            "Trainable params: 6,048\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifNceeUhzaRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Won't be able to add a third pair of conv-pooling layers\n",
        "model.add(Conv2D(filters=72, kernel_size=(3, 3), activation='tanh', padding='same', \n",
        "                 data_format='channels_last'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7RjD2QY3RYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a71e67be-2339-4a72-9ed4-2fab84556868"
      },
      "source": [
        "# After the conv-layer\n",
        "model.summary()"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_47 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 74, 3, 18)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 74, 3, 36)         5868      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 37, 1, 36)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 37, 1, 72)         23400     \n",
            "=================================================================\n",
            "Total params: 29,448\n",
            "Trainable params: 29,448\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWxs9Ke63Tqr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "abbcf793-4bda-4387-a3fc-39ffc5a1a360"
      },
      "source": [
        "# Add a pooling layer?\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1606\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1607\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1608\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_20/MaxPool' (op: 'MaxPool') with input shapes: [?,37,1,72].",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-214-1308ef673ad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/pooling.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    203\u001b[0m                                         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                                         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                                         data_format=self.data_format)\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/pooling.py\u001b[0m in \u001b[0;36m_pooling_function\u001b[0;34m(self, inputs, pool_size, strides, padding, data_format)\u001b[0m\n\u001b[1;32m    266\u001b[0m         output = K.pool2d(inputs, pool_size, strides,\n\u001b[1;32m    267\u001b[0m                           \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                           pool_mode='max')\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mpool2d\u001b[0;34m(x, pool_size, strides, padding, data_format, pool_mode)\u001b[0m\n\u001b[1;32m   4267\u001b[0m         x = tf.nn.max_pool(x, pool_size, strides,\n\u001b[1;32m   4268\u001b[0m                            \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4269\u001b[0;31m                            data_format=tf_data_format)\n\u001b[0m\u001b[1;32m   4270\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpool_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'avg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4271\u001b[0m         x = tf.nn.avg_pool(x, pool_size, strides,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(value, ksize, strides, padding, data_format, name, input)\u001b[0m\n\u001b[1;32m   3813\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3815\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   3816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   5672\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5673\u001b[0m         \u001b[0;34m\"MaxPool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5674\u001b[0;31m                    data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m   5675\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5676\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    792\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    793\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0;31m# Conditionally invoke tfdbg v2's op callback(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3355\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input #%d is not a tensor: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3356\u001b[0m     return self._create_op_internal(op_type, inputs, dtypes, input_types, name,\n\u001b[0;32m-> 3357\u001b[0;31m                                     attrs, op_def, compute_device)\n\u001b[0m\u001b[1;32m   3358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3359\u001b[0m   def _create_op_internal(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3424\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3425\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3426\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3427\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1768\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1769\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1770\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1771\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_20/MaxPool' (op: 'MaxPool') with input shapes: [?,37,1,72]."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09RoRzU13YQt",
        "colab_type": "text"
      },
      "source": [
        "Cannot add another `MaxPooling2D` layer. There input tensor is too small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpXSdKl57v7E",
        "colab_type": "text"
      },
      "source": [
        "# CNN 2 - One Pooling Layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yISvb-Zx7yql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Available in the tensorflow distribution of keras only\n",
        "from tensorflow.keras.metrics import AUC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOvdHZ7A74dQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new model\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keOcOm8B-TgY",
        "colab_type": "text"
      },
      "source": [
        "## Conv1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ2nbW797_Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add first convolutional layer\n",
        "model.add(Conv2D(filters=18, input_shape=(148, 7, 1), kernel_size=(3, 3), padding='same',\n",
        "                 data_format='channels_last', activation='tanh'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px9KAnwX8MJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "82d5fcc8-feb2-403c-ac53-792851c02d84"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "=================================================================\n",
            "Total params: 180\n",
            "Trainable params: 180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0aBvyXk-WQV",
        "colab_type": "text"
      },
      "source": [
        "## Conv2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EO8MxJ58mKW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "09b744fe-547e-4630-e009-23a0445162f5"
      },
      "source": [
        "# Add second convolutional layer\n",
        "model.add(Conv2D(filters=18, kernel_size=(3, 3), padding='same',\n",
        "                 data_format='channels_last', activation='tanh'))\n",
        "model.summary()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "=================================================================\n",
            "Total params: 3,114\n",
            "Trainable params: 3,114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K06YYai-YSq",
        "colab_type": "text"
      },
      "source": [
        "## Conv3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY-c7AHN8whs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "895bcee0-611f-413f-c1fc-58d2407ea292"
      },
      "source": [
        "# Add a third convolutional layer\n",
        "model.add(Conv2D(filters=18, kernel_size=(3, 3), padding='same', \n",
        "                 data_format='channels_last', activation='tanh'))\n",
        "model.summary()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "=================================================================\n",
            "Total params: 6,048\n",
            "Trainable params: 6,048\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbaWpe_c-Z0l",
        "colab_type": "text"
      },
      "source": [
        "## Conv4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbmjHlOS8-4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8e207c7d-76f8-41c3-ab0c-7f390987f3c3"
      },
      "source": [
        "# Add a fourth convolutional layer\n",
        "model.add(Conv2D(filters=18, kernel_size=(3, 3), padding='same', \n",
        "                 data_format='channels_last', activation='tanh'))\n",
        "model.summary()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "=================================================================\n",
            "Total params: 8,982\n",
            "Trainable params: 8,982\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER0TNd2I-bmi",
        "colab_type": "text"
      },
      "source": [
        "## Conv5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrPBYYgG9CCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "92d10421-9b8e-425a-ac04-12c37d1403c1"
      },
      "source": [
        "# Add a fifth convolutional layer\n",
        "model.add(Conv2D(filters=18, kernel_size=(3, 3), padding='same', \n",
        "                 data_format='channels_last', activation='tanh'))\n",
        "model.summary()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "=================================================================\n",
            "Total params: 11,916\n",
            "Trainable params: 11,916\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeF-x_bF-dMA",
        "colab_type": "text"
      },
      "source": [
        "## Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n379iOQH9lXE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "733542d2-717a-4db0-9812-60c1b252f44e"
      },
      "source": [
        "# Add a single max pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(7, 7)))\n",
        "model.summary()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 21, 1, 18)         0         \n",
            "=================================================================\n",
            "Total params: 11,916\n",
            "Trainable params: 11,916\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GL8Qc3M-1He",
        "colab_type": "text"
      },
      "source": [
        "## Densely Connected Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8iCc3jg_ZEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "d59c0d16-35c8-4336-9be5-4acb9cbb14cd"
      },
      "source": [
        "# Must flatten the model first \n",
        "model.add(Flatten())\n",
        "model.summary()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 21, 1, 18)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 378)               0         \n",
            "=================================================================\n",
            "Total params: 11,916\n",
            "Trainable params: 11,916\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B0651xY_oNf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "4cbe0f98-366f-453b-8338-db18c034c05f"
      },
      "source": [
        "# Zheng's results show max AUC with ~64 neurons at training ratio of 0.8\n",
        "# Described as 'similar to wide component' so activation chosen as relu\n",
        "model.add(Dense(units=64, activation='relu'))    \n",
        "model.summary()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 21, 1, 18)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 378)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                24256     \n",
            "=================================================================\n",
            "Total params: 36,172\n",
            "Trainable params: 36,172\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuC2qUkUCqiA",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid Activation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHInGo9vCp8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIDk9ajCCp41",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "279acaae-9be3-468d-96c8-1e352ad2c4d3"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 148, 7, 18)        180       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 148, 7, 18)        2934      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 21, 1, 18)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 378)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                24256     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 36,237\n",
            "Trainable params: 36,237\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At0w2bVmC7LW",
        "colab_type": "text"
      },
      "source": [
        "## Compiling the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdDZkCjh9D1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop',               # Unspecified in paper. Assumed\n",
        "              loss='binary_crossentropy')        # Log loss\n",
        "# AUC needs a little more work than just instantiating an object.\n",
        "# Go over the documentation with supervisor\n",
        "              # metrics=[AUC()])                   # Only available in tf distribution of keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O14c5BRHAa74",
        "colab_type": "text"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh7Gw8bB9b7I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "089476c1-ff07-4c89-d455-7e622c4cbcd8"
      },
      "source": [
        "# Train the model\n",
        "model_history = model.fit(x=X_train, y=y_train,      # Training data \n",
        "                          batch_size=128,            # Unspecified in paper. Assumed\n",
        "                          epochs=20,                 # From figure 9b\n",
        "                          validation_split=0.2,      # Unspecified. Assumed\n",
        "                          verbose=1)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 27117 samples, validate on 6780 samples\n",
            "Epoch 1/20\n",
            "27117/27117 [==============================] - 3s 107us/step - loss: 0.2867 - val_loss: 0.2835\n",
            "Epoch 2/20\n",
            "27117/27117 [==============================] - 2s 85us/step - loss: 0.2680 - val_loss: 0.2717\n",
            "Epoch 3/20\n",
            "27117/27117 [==============================] - 2s 81us/step - loss: 0.2574 - val_loss: 0.2553\n",
            "Epoch 4/20\n",
            "27117/27117 [==============================] - 2s 80us/step - loss: 0.2507 - val_loss: 0.2584\n",
            "Epoch 5/20\n",
            "27117/27117 [==============================] - 2s 81us/step - loss: 0.2455 - val_loss: 0.2541\n",
            "Epoch 6/20\n",
            "27117/27117 [==============================] - 2s 81us/step - loss: 0.2398 - val_loss: 0.2538\n",
            "Epoch 7/20\n",
            "27117/27117 [==============================] - 2s 79us/step - loss: 0.2360 - val_loss: 0.2555\n",
            "Epoch 8/20\n",
            "27117/27117 [==============================] - 2s 81us/step - loss: 0.2319 - val_loss: 0.2582\n",
            "Epoch 9/20\n",
            "27117/27117 [==============================] - 2s 80us/step - loss: 0.2278 - val_loss: 0.2510\n",
            "Epoch 10/20\n",
            "27117/27117 [==============================] - 2s 80us/step - loss: 0.2226 - val_loss: 0.2654\n",
            "Epoch 11/20\n",
            "27117/27117 [==============================] - 2s 83us/step - loss: 0.2177 - val_loss: 0.2602\n",
            "Epoch 12/20\n",
            "27117/27117 [==============================] - 2s 82us/step - loss: 0.2120 - val_loss: 0.2574\n",
            "Epoch 13/20\n",
            "27117/27117 [==============================] - 2s 84us/step - loss: 0.2078 - val_loss: 0.2624\n",
            "Epoch 14/20\n",
            "27117/27117 [==============================] - 2s 82us/step - loss: 0.2027 - val_loss: 0.2575\n",
            "Epoch 15/20\n",
            "27117/27117 [==============================] - 2s 81us/step - loss: 0.1965 - val_loss: 0.2802\n",
            "Epoch 16/20\n",
            "27117/27117 [==============================] - 2s 80us/step - loss: 0.1923 - val_loss: 0.2670\n",
            "Epoch 17/20\n",
            "27117/27117 [==============================] - 2s 81us/step - loss: 0.1859 - val_loss: 0.2761\n",
            "Epoch 18/20\n",
            "27117/27117 [==============================] - 2s 87us/step - loss: 0.1813 - val_loss: 0.2766\n",
            "Epoch 19/20\n",
            "27117/27117 [==============================] - 2s 80us/step - loss: 0.1752 - val_loss: 0.2758\n",
            "Epoch 20/20\n",
            "27117/27117 [==============================] - 2s 80us/step - loss: 0.1698 - val_loss: 0.2760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opbvM6QABvkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history, model_name, has_metrics=False, metric_name=None, \n",
        "                 loss_metric=None):\n",
        "  \"\"\"Plots training and validation loss, as well as optional metric\"\"\"\n",
        "  # Extract loss from the history object\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = np.arange(1, len(loss) + 1)\n",
        "\n",
        "  # Create a new figure for plotting loss\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.plot(epochs, loss, label='Training Loss')\n",
        "  plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "  plt.xlabel('Epochs', fontsize=14)\n",
        "  plt.ylabel('Loss' if loss_metric is not None else loss_metric, fontsize=14)\n",
        "  plt.title('Training vs Validation Loss \\n{}'.format('' if model_name is None else model_name), \n",
        "            fontsize=18)\n",
        "  plt.legend()\n",
        "\n",
        "  # If the user has also asked to plot a metric\n",
        "  if has_metrics and metric_name is not None:\n",
        "    # Create a new figure for plotting the metric\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    metric = history.history[metric_name]\n",
        "    val_metric = history.history['val_{}'.format_metric_name]\n",
        "    \n",
        "    plt.plot(epochs, metric, label='Training')\n",
        "    plt.plot(epochs, val_metric, label='Validation')\n",
        "    plt.xlabel('Epochs', fontsize=14)\n",
        "    plt.ylabel(metric_name, fontsize=14)\n",
        "    plt.title('Training vs Validation Loss {}'.format(metric_name), fontsize=18)\n",
        "    plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tujWLkk8Cdcx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "4c99b2f2-a769-4a3a-9407-18eff5172974"
      },
      "source": [
        "plot_history(history=model_history, \n",
        "             model_name='One Pooling Layer, 18 CNN neurons, 64 classifier neurons')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAGgCAYAAABYJvriAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}